import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import pandas as pd
import time
import random
import os
import re
from datetime import datetime
import numpy as np

# BERT & Clustering
# [Warning Fix] ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ë„ê¸°
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import CountVectorizer
from kiwipiepy import Kiwi

# ==========================================
# [ì„¤ì •] íŒŒë¼ë¯¸í„°
# ==========================================
NUM_CLUSTERS = 3           # ê¸°ë³¸ êµ°ì§‘(í† í”½) ê°œìˆ˜ (AUTO_CLUSTER=Falseì¼ ë•Œ ì‚¬ìš©)
KEYWORDS_PER_CLUSTER = 5    # êµ°ì§‘ë‹¹ í‚¤ì›Œë“œ ìˆ˜
MODEL_NAME = 'jhgan/ko-sroberta-multitask' # í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©ì— ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸ (SBERT)

# í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ìë™ ê²°ì • (Silhouette Score ì‚¬ìš©)
AUTO_CLUSTER = True        # Trueë©´ ìë™ìœ¼ë¡œ ìµœì  í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ íƒìƒ‰
CLUSTER_RANGE = (5, 30)     # íƒìƒ‰í•  í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ë²”ìœ„ (ìµœì†Œ 3ê°œ ì´ìƒ ê¶Œì¥)

# ë¶ˆìš©ì–´ (DCì¸ì‚¬ì´ë“œ ë…¸ì´ì¦ˆ ëŒ€ì‘ ê°•í™”)
STOPWORDS = [
    # ë‰´ìŠ¤ ê´€ë ¨
    'ë‰´ìŠ¤', 'ì†ë³´', 'ë‹¨ë…', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ë°œí‘œ', 'ê³µê°œ', 
    'ì˜ìƒ', 'ë…¼ë€', 'ì´ìœ ', 'ì¶©ê²©', 'ê²°êµ­', 'ì§„ì§œ', 'ê·¼í™©', 'ì˜ˆì •',
    'ê´€ë ¨', 'íŠ¹ì§•', 'ê°€ì¥', 'ëŒ€í•´', 'ìœ„í•´', 'í†µí•´', 'ë•Œë¬¸', 'ê²½ìš°',
    'ì •ë„', 'ìµœê·¼', 'ì§€ê¸ˆ', 'ë¬´ì—‡', 'ì–´ë–»ê²Œ', 'ë‹¤ì‹œ', 'ê³„ì†', 'ì¢…í•©',
    'ì¶œì‹œ', 'ê³µì‹', 'ì „ë§', 'ë¶„ì„', 'ì‹œì¥', 'ì„¸ê³„', 'êµ­ë‚´', 'í•œêµ­',
    'ì£¼ìš”', 'ìµœê³ ', 'ëŒ€ë¹„', 'ì‹œì‘', 'ê°œìµœ', 'ì§„í–‰', 'ì°¸ì—¬', 'ë“±ì¥',
    'ì •ë‹µ', 'í€´ì¦ˆ', 'ë¬¸ì œ', 'ì´ë²¤íŠ¸', 'ë‹¹ì²¨', 'ì°¸ê°€',
    # DC ë…¸ì´ì¦ˆ (ìš•ì„¤/ë¹„ì†ì–´/ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´)
    'ìƒˆë¼', 'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°', 'ë­ì„', 'ê°œë…', 'í”¼í‹°', 'ìƒê°', 'ì‚¬ëŒ',
    'ì§„ì§œ', 'ì •ë§', 'ë§¤ìš°', 'ë„ˆë¬´', 'ì™„ì „', 'ëŒ€ë°•', 'í—', 'ì™€', 'ìš°ì™€',
    'ëŒ“ê¸€', 'ì¶”ì²œ', 'ë¹„ì¶”', 'ì‹ ê³ ', 'ì‚­ì œ', 'ì°¨ë‹¨', 'ê¸€ì“´ì´', 'ì‘ì„±ì',
    'í˜•ë“¤', 'ìê²Œ', 'ì—¬ê¸°', 'ì €ê¸°', 'ê·¸ëƒ¥', 'ì´ì œ', 'ì´ë¯¸', 'ë²Œì¨',
    'í•´ë§ˆ', 'ì•„ë¹ ', 'ì •ì‹ ', 'ëŠë‚Œ', 'ã…‹ã…‹', 'ì•ˆë…•', 'ã…‡ã…‡', 'ã„·ã„·','ì§€ë„','ì •ë³‘','í•´ì£¼ê°¤','ì´ì¬ëª…'
]

# Kiwi ì´ˆê¸°í™”
try:
    kiwi = Kiwi()
    # [ì¤‘ìš”] ì‚¬ìš©ì ì‚¬ì „ ì¶”ê°€ (ì˜ëª» ë¶„ë¦¬ë˜ëŠ” ì‹ ì¡°ì–´/ê³ ìœ ëª…ì‚¬ ë“±ë¡)
    kiwi.add_user_word("ì œë¯¸ë‚˜ì´", "NNP") 
    kiwi.add_user_word("ì±—GPT", "NNP")
    kiwi.add_user_word("ë°”ì´ë¸Œì½”ë”©", "NNP")
    print("[System] Kiwi ë¡œë“œ ì„±ê³µ. (ì‚¬ìš©ì ì‚¬ì „ ì ìš©ë¨)")
except:
    kiwi = None
    print("[System] Kiwi ë¡œë“œ ì‹¤íŒ¨. (pip install kiwipiepy)")

# ==========================================
# 1. ë°ì´í„° ìˆ˜ì§‘ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
# ==========================================
def crawl_dcinside(pages=0):
    """DCì¸ì‚¬ì´ë“œ ì¸ê¸° ê°¤ëŸ¬ë¦¬ì—ì„œ ê°œë…ê¸€ ì œëª© ìˆ˜ì§‘"""
    galleries = [
        ('programming', 'ê°œë°œì', 'major'),
        ('chatgpt', 'AI/ChatGPT', 'minor'),
        ('robot', 'ë¡œë´‡', 'minor')
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://www.dcinside.com/'
    }
    
    titles = []
    base_url = "https://gall.dcinside.com"
    
    # í•„í„°ë§í•  íŒ¨í„´ (ë…¸ì´ì¦ˆ ì œëª©)
    noise_patterns = ['ã…‹', 'ã…', 'ã„·', 'ã…‡ã…‡', '...', '??', '!!']
    
    for g_id, g_name, g_type in galleries:
        for page in range(1, pages + 1):
            try:
                # ê°œë…ê¸€(ì¶”ì²œìˆœ) URL
                if g_type == 'major':
                    url = f"{base_url}/board/lists/?id={g_id}&exception_mode=recommend&page={page}"
                else:
                    url = f"{base_url}/mgallery/board/lists/?id={g_id}&exception_mode=recommend&page={page}"
                
                res = requests.get(url, headers=headers, timeout=10)
                if res.status_code != 200: continue
                
                soup = BeautifulSoup(res.text, 'html.parser')
                rows = soup.select('.ub-content.us-post')
                if not rows:
                    rows = soup.select('tr.ub-content')
                
                for row in rows[:30]:  # í˜ì´ì§€ë‹¹ ìµœëŒ€ 30ê°œ
                    try:
                        title_tag = row.select_one('.gall_tit a')
                        if title_tag:
                            title = title_tag.text.strip()
                            
                            # í•„í„°ë§: ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” ì œëª© ì œì™¸
                            if len(title) < 5:  # 5ì ë¯¸ë§Œ ì œì™¸
                                continue
                            if any(pattern * 2 in title for pattern in noise_patterns):  # 'ã…‹ã…‹', 'ã…ã…' ë“± ë°˜ë³µ ì œì™¸
                                continue
                            if title.replace(' ', '').replace('ã…‹', '').replace('ã…', '').replace('ã„·', '').replace('ã…‡', '') == '':  # ììŒë§Œ ìˆëŠ” ê²½ìš°
                                continue
                            
                            titles.append(title)
                    except: pass
                
                time.sleep(random.uniform(1.0, 2.0))
            except: pass
    
    return titles

def collect_data():
    print("\n" + "="*60)
    print("ğŸ“¡ [Phase 1] ë°ì´í„° ìˆ˜ì§‘ (ê¸°ìˆ  ë¸”ë¡œê·¸ RSS)")
    print("="*60)
    
    raw_data = []
    
    # ê¸°ìˆ  ë¸”ë¡œê·¸ RSSë§Œ ìˆ˜ì§‘ (ë…¸ì´ì¦ˆ ìµœì†Œí™”)
    print("   -> ê¸°ìˆ  ë¸”ë¡œê·¸ + ê¸±ë‰´ìŠ¤ RSS ìˆ˜ì§‘ ì¤‘...")
    headers = {"User-Agent": "Mozilla/5.0"}
    
    # ë‰´ìŠ¤ RSS (ê¸±ë‰´ìŠ¤ë§Œ)
    news_rss = [
        ('ê¸±ë‰´ìŠ¤', 'https://feeds.feedburner.com/geeknews-feed'),
    ]
    
    # í•œêµ­ IT ê¸°ì—… ê¸°ìˆ  ë¸”ë¡œê·¸ RSS
    tech_blog_rss = [
        ('ë¬´ì‹ ì‚¬', 'https://medium.com/feed/musinsa-tech'),
        ('ë„¤ì´ë²„ D2', 'https://d2.naver.com/d2.atom'),
        ('ë§ˆì¼“ì»¬ë¦¬', 'https://helloworld.kurly.com/feed.xml'),
        ('ìš°ì•„í•œí˜•ì œë“¤', 'https://techblog.woowahan.com/feed'),
        ('ì¹´ì¹´ì˜¤ì—”í„°', 'https://tech.kakaoenterprise.com/feed'),
        ('ë°ë¸Œì‹œìŠ¤í„°ì¦ˆ', 'https://tech.devsisters.com/rss.xml'),
        ('ë¼ì¸', 'https://engineering.linecorp.com/ko/feed/index.html'),
        ('ì¿ íŒ¡', 'https://medium.com/feed/coupang-engineering'),
        ('ë‹¹ê·¼ë§ˆì¼“', 'https://medium.com/feed/daangn'),
        ('í† ìŠ¤', 'https://toss.tech/rss.xml'),
        ('ì§ë°©', 'https://medium.com/feed/zigbang'),
        ('ì™“ì± ', 'https://medium.com/feed/watcha'),
        ('ë±…í¬ìƒëŸ¬ë“œ', 'https://blog.banksalad.com/rss.xml'),
        ('Hyperconnect', 'https://hyperconnect.github.io/feed.xml'),
        ('ìš”ê¸°ìš”', 'https://techblog.yogiyo.co.kr/feed'),
        ('ì˜ì¹´', 'https://tech.socarcorp.kr/feed'),
        ('ë¦¬ë””', 'https://www.ridicorp.com/feed'),
        ('NHN Toast', 'https://meetup.toast.com/rss'),
        ('Velog', 'https://v2.velog.io/rss/'),
        ('ê°œë°œììŠ¤ëŸ½ë‹¤', 'https://blog.gaerae.com/feeds/posts/default?alt=rss'),
        ('44BITS', 'https://www.44bits.io/ko/feed/all'),
    ]
    
    all_rss = news_rss + tech_blog_rss
    
    for name, url in all_rss:
        try:
            res = requests.get(url, headers=headers, timeout=10)
            root = ET.fromstring(res.content)
            count = 0
            
            # Atom í˜•ì‹ (ê¸±ë‰´ìŠ¤, ë„¤ì´ë²„ D2, Hyperconnect ë“±)
            if any(x in url for x in ['geeknews', 'd2.naver', 'hyperconnect']):
                for item in root.findall(".//{http://www.w3.org/2005/Atom}entry"):
                    try:
                        title_elem = item.find("{http://www.w3.org/2005/Atom}title")
                        if title_elem is not None and title_elem.text:
                            raw_data.append(title_elem.text)
                            count += 1
                    except: pass
            
            # RSS 2.0 í˜•ì‹
            if count == 0:
                for item in root.findall(".//item"):
                    try:
                        title_elem = item.find("title")
                        if title_elem is not None and title_elem.text:
                            title = title_elem.text
                            raw_data.append(title)
                            count += 1
                    except: pass
            
            if count > 0:
                print(f"      -> {name}: {count}ê°œ")
        except Exception as e:
            # ì—ëŸ¬ëŠ” ì¡°ìš©íˆ ë„˜ê¹€ (ì¼ë¶€ ë¸”ë¡œê·¸ëŠ” ì—…ë°ì´íŠ¸ê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
            pass
    
    unique_data = list(set(raw_data))
    print(f"   -> ì´ {len(unique_data)}ê°œ ë¬¸ì¥ í™•ë³´.")
    return unique_data

# ==========================================
# 2. BERT ì„ë² ë”© ë° í´ëŸ¬ìŠ¤í„°ë§
# ==========================================
def extract_keywords_with_bert(docs):
    print("\n" + "="*60)
    print(f"ğŸ§  [Phase 2] KoBERT(SBERT) ì„ë² ë”© & í´ëŸ¬ìŠ¤í„°ë§")
    print("="*60)
    
    if not docs: return []

    # 1. ì„ë² ë”© (Vectorization)
    print(f"   -> ëª¨ë¸ ë¡œë“œ ì¤‘ ({MODEL_NAME})...")
    model = SentenceTransformer(MODEL_NAME)
    
    print("   -> ë¬¸ì¥ ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = model.encode(docs, show_progress_bar=True)
    
    # 2. í´ëŸ¬ìŠ¤í„°ë§ (K-Means)
    # í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ìë™ ê²°ì • (Grid Search)
    if AUTO_CLUSTER:
        print(f"   -> ìµœì  í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ íƒìƒ‰ ì¤‘ (ë²”ìœ„: {CLUSTER_RANGE[0]}-{CLUSTER_RANGE[1]})...")
        best_score = -1
        best_n_clusters = NUM_CLUSTERS
        scores = {}
        
        for n in range(CLUSTER_RANGE[0], CLUSTER_RANGE[1] + 1):
            kmeans = KMeans(n_clusters=n, random_state=42, n_init=10)
            labels = kmeans.fit_predict(embeddings)
            score = silhouette_score(embeddings, labels)
            scores[n] = score
            
            if score > best_score:
                best_score = score
                best_n_clusters = n
        
        print(f"   -> Silhouette Scores: {scores}")
        print(f"   -> âœ… ìµœì  í´ëŸ¬ìŠ¤í„° ê°œìˆ˜: {best_n_clusters} (Score: {best_score:.3f})")
        
        num_clusters = best_n_clusters
    else:
        num_clusters = NUM_CLUSTERS
        print(f"   -> í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (Clusters={num_clusters})...")
    
    # ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§
    clustering_model = KMeans(n_clusters=num_clusters, random_state=42)
    clustering_model.fit(embeddings)
    cluster_assignment = clustering_model.labels_
    
    # 3. í´ëŸ¬ìŠ¤í„°ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ (c-TF-IDF ë°©ì‹ í‰ë‚´)
    # ê° í´ëŸ¬ìŠ¤í„°ì˜ ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ ê¸´ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
    clustered_docs = {i: [] for i in range(num_clusters)}
    for sentence_id, cluster_id in enumerate(cluster_assignment):
        clustered_docs[cluster_id].append(docs[sentence_id])
        
    final_keywords = []  # setì—ì„œ listë¡œ ë³€ê²½ (í´ëŸ¬ìŠ¤í„° ì •ë³´ í¬í•¨í•˜ê¸° ìœ„í•´)
    
    print("\n   ğŸ” [Cluster Analysis Results]")
    
    # ëª…ì‚¬ ì¶”ì¶œ í† í¬ë‚˜ì´ì €
    def tokenizer(text):
        if kiwi:
            tokens = kiwi.tokenize(text)
            nouns = [t.form for t in tokens if t.tag.startswith('N')]
            return [n for n in nouns if len(n) >= 2 and n not in STOPWORDS]
        else:
            return text.split()

    for i in range(num_clusters):
        sentences = clustered_docs[i]
        if not sentences: continue
        
        # í´ëŸ¬ìŠ¤í„° ë‚´ ë¬¸ì¥ë“¤ì„ í•©ì³ì„œ ë¹ˆë„ ë¶„ì„
        combined_text = " ".join(sentences)
        
        # CountVectorizerë¡œ ë¹ˆë„ ë†’ì€ ëª…ì‚¬ ì¶”ì¶œ
        try:
            # [Warning Fix] tokenizer ì‚¬ìš© ì‹œ token_pattern=None ì„¤ì •
            cv = CountVectorizer(tokenizer=tokenizer, token_pattern=None, max_features=10)
            cv.fit([combined_text])
            top_words = list(cv.vocabulary_.keys())[:KEYWORDS_PER_CLUSTER]
            
            print(f"\n   ğŸ“‚ Cluster {i+1} (ë¬¸ì¥ {len(sentences)}ê°œ)")
            print(f"      - ëŒ€í‘œ ë¬¸ì¥: {sentences[0][:40]}...")
            print(f"      - í‚¤ì›Œë“œ: {top_words}")
            
            # í´ëŸ¬ìŠ¤í„° ì •ë³´ì™€ í•¨ê»˜ ì €ì¥
            for word in top_words:
                final_keywords.append({'keyword': word, 'cluster': i+1})
        except:
            pass

    return final_keywords

# ==========================================
# ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    # 1. ìˆ˜ì§‘
    docs = collect_data()
    
    # 2. ë¶„ì„
    keywords = extract_keywords_with_bert(docs)
    
    # 3. ì €ì¥
    if keywords:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        filename = f"trend_keywords_bert_{timestamp}.csv"
        
        # DataFrameìœ¼ë¡œ ì €ì¥ (keyword, cluster ì»¬ëŸ¼ í¬í•¨)
        df = pd.DataFrame(keywords)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print("\n" + "="*60)
        print(f"ğŸ‰ BERT ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“ íŒŒì¼ëª…: {filename}")
        print(f"ğŸ”‘ ì¶”ì¶œ í‚¤ì›Œë“œ: {len(keywords)}ê°œ")
        print("="*60)

if __name__ == "__main__":
    main()
