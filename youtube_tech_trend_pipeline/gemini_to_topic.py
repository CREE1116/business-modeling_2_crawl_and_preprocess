import google.generativeai as genai
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import pandas as pd
import time
import random
import os
import re
from datetime import datetime
import json
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ==========================================
# [ì„¤ì •] API í‚¤ ë° íŒŒë¼ë¯¸í„°
# ==========================================
# .env íŒŒì¼ì—ì„œ API í‚¤ ì½ê¸°
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    raise ValueError("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”!") 

# ìˆ˜ì§‘í•  í˜ì´ì§€ ìˆ˜ (DCì¸ì‚¬ì´ë“œ)
DC_PAGES = 2

# Gemini ëª¨ë¸ ì„¤ì •
genai.configure(api_key=GEMINI_API_KEY)
# ìµœì‹  íŠ¸ë Œë“œ ë¶„ì„ì—” ì†ë„ê°€ ë¹ ë¥¸ Flash ëª¨ë¸ì´ ì í•©í•©ë‹ˆë‹¤.
model = genai.GenerativeModel('gemini-1.5-pro')

# ==========================================
# 1. ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ
# ==========================================
def crawl_dcinside(pages=3):
    """DCì¸ì‚¬ì´ë“œ ì£¼ìš” ê°¤ëŸ¬ë¦¬ ê°œë…ê¸€ ìˆ˜ì§‘"""
    galleries = [
        ('programming', 'ê°œë°œì', 'major'),
        ('chatgpt', 'AI/ChatGPT', 'minor'),
        ('robot', 'ë¡œë´‡', 'minor')
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://www.dcinside.com/'
    }
    
    titles = []
    base_url = "https://gall.dcinside.com"
    
    # ë…¸ì´ì¦ˆ í•„í„°ë§ íŒ¨í„´
    noise_patterns = ['ã…‹', 'ã…', 'ã„·', 'ã…‡ã…‡', '...', '??', '!!']
    
    print(f"   -> DCì¸ì‚¬ì´ë“œ {len(galleries)}ê°œ ê°¤ëŸ¬ë¦¬ ìˆœíšŒ ì¤‘...")
    
    for g_id, g_name, g_type in galleries:
        for page in range(1, pages + 1):
            try:
                if g_type == 'major':
                    url = f"{base_url}/board/lists/?id={g_id}&exception_mode=recommend&page={page}"
                else:
                    url = f"{base_url}/mgallery/board/lists/?id={g_id}&exception_mode=recommend&page={page}"
                
                res = requests.get(url, headers=headers, timeout=5)
                if res.status_code != 200: continue
                
                soup = BeautifulSoup(res.text, 'html.parser')
                rows = soup.select('.ub-content.us-post')
                if not rows: rows = soup.select('tr.ub-content')
                
                for row in rows:
                    try:
                        title_tag = row.select_one('.gall_tit a')
                        if title_tag:
                            title = title_tag.text.strip()
                            # ê°„ë‹¨í•œ ì „ì²˜ë¦¬
                            if len(title) < 2: continue
                            if any(p * 3 in title for p in noise_patterns): continue # ã…‹ã…‹ã…‹ ë°˜ë³µ ë“±
                            titles.append(f"[{g_name}] {title}") # ê°¤ëŸ¬ë¦¬ ì´ë¦„ í¬í•¨í•´ì„œ ë¬¸ë§¥ ì œê³µ
                    except: pass
                
                time.sleep(random.uniform(0.5, 1.0)) # ì°¨ë‹¨ ë°©ì§€
            except Exception as e:
                print(f"      [Error] {g_name}: {e}")
                
    return list(set(titles))

def collect_data():
    """RSS ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ (Selenium ë¶ˆí•„ìš”)"""
    print("\n" + "="*60)
    print("ğŸ“¡ [Phase 1] ë°ì´í„° ìˆ˜ì§‘ (ê¸°ìˆ  ë¸”ë¡œê·¸ RSS)")
    print("="*60)
    
    raw_data = []
    headers = {"User-Agent": "Mozilla/5.0"}
    
    # ë‰´ìŠ¤ RSS
    news_rss = [
        ('ê¸±ë‰´ìŠ¤', 'https://feeds.feedburner.com/geeknews-feed'),
    ]
    
    # í•œêµ­ IT ê¸°ì—… ê¸°ìˆ  ë¸”ë¡œê·¸ RSS
    tech_blog_rss = [
        ('ë¬´ì‹ ì‚¬', 'https://medium.com/feed/musinsa-tech'),
        ('ë„¤ì´ë²„ D2', 'https://d2.naver.com/d2.atom'),
        ('ë§ˆì¼“ì»¬ë¦¬', 'https://helloworld.kurly.com/feed.xml'),
        ('ìš°ì•„í•œí˜•ì œë“¤', 'https://techblog.woowahan.com/feed'),
        ('ì¹´ì¹´ì˜¤ì—”í„°', 'https://tech.kakaoenterprise.com/feed'),
        ('ë°ë¸Œì‹œìŠ¤í„°ì¦ˆ', 'https://tech.devsisters.com/rss.xml'),
        ('ë¼ì¸', 'https://engineering.linecorp.com/ko/feed/index.html'),
        ('ì¿ íŒ¡', 'https://medium.com/feed/coupang-engineering'),
        ('ë‹¹ê·¼ë§ˆì¼“', 'https://medium.com/feed/daangn'),
        ('í† ìŠ¤', 'https://toss.tech/rss.xml'),
        ('ì§ë°©', 'https://medium.com/feed/zigbang'),
        ('ì™“ì± ', 'https://medium.com/feed/watcha'),
        ('ë±…í¬ìƒëŸ¬ë“œ', 'https://blog.banksalad.com/rss.xml'),
        ('Hyperconnect', 'https://hyperconnect.github.io/feed.xml'),
        ('ìš”ê¸°ìš”', 'https://techblog.yogiyo.co.kr/feed'),
        ('ì˜ì¹´', 'https://tech.socarcorp.kr/feed'),
        ('ë¦¬ë””', 'https://www.ridicorp.com/feed'),
        ('NHN Toast', 'https://meetup.toast.com/rss'),
        ('Velog', 'https://v2.velog.io/rss/'),
        ('ê°œë°œììŠ¤ëŸ½ë‹¤', 'https://blog.gaerae.com/feeds/posts/default?alt=rss'),
        ('44BITS', 'https://www.44bits.io/ko/feed/all'),
    ]
    
    all_rss = news_rss + tech_blog_rss
    
    print("   -> RSS ìˆ˜ì§‘ ì¤‘...")
    for name, url in all_rss:
        try:
            res = requests.get(url, headers=headers, timeout=10)
            root = ET.fromstring(res.content)
            count = 0
            
            # Atom í˜•ì‹
            if any(x in url for x in ['geeknews', 'd2.naver', 'hyperconnect']):
                for item in root.findall(".//{http://www.w3.org/2005/Atom}entry"):
                    try:
                        title_elem = item.find("{http://www.w3.org/2005/Atom}title")
                        if title_elem is not None and title_elem.text:
                            raw_data.append(f"[{name}] {title_elem.text}")
                            count += 1
                    except: pass
            
            # RSS 2.0 í˜•ì‹
            if count == 0:
                for item in root.findall(".//item"):
                    try:
                        title_elem = item.find("title")
                        if title_elem is not None and title_elem.text:
                            raw_data.append(f"[{name}] {title_elem.text}")
                            count += 1
                    except: pass
            
            if count > 0:
                print(f"      -> {name}: {count}ê°œ")
        except:
            pass
    
    print(f"   -> ì´ {len(raw_data)}ê°œ í…ìŠ¤íŠ¸ í™•ë³´.\n")
    return raw_data

# ==========================================
# 2. Gemini ë¶„ì„ ëª¨ë“ˆ (BERT ëŒ€ì²´)
# ==========================================
def extract_keywords_with_gemini(docs):
    print("\n" + "="*60)
    print(f"ğŸ§  [Phase 2] Gemini 1.5 Flash íŠ¸ë Œë“œ ë¶„ì„")
    print("="*60)
    
    if not docs: return []
    
    # ë„ˆë¬´ ë§ì€ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë³´ë‚´ë©´ í† í° ì œí•œì— ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒ˜í”Œë§í•˜ê±°ë‚˜ ë°°ì¹˜ë¥¼ ë‚˜ëˆ”
    # ì—¬ê¸°ì„œëŠ” ìµœì‹  ë°ì´í„° ìœ„ì£¼ë¡œ ìµœëŒ€ 500ê°œë§Œ ì¶”ë ¤ì„œ ë³´ëƒ„ (Flash ëª¨ë¸ì€ 100ë§Œ í† í°ê¹Œì§€ ê°€ëŠ¥í•˜ê¸´ í•¨)
    docs_sample = docs[:2000] 
    
    doc_text = "\n".join(docs_sample)
    
    # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ (í•µì‹¬) - reason ì»¬ëŸ¼ ì œê±°í•˜ì—¬ íŒŒì‹± ì—ëŸ¬ ë°©ì§€
    prompt = f"""
# ì—­í• 
ë‹¹ì‹ ì€ IT ê¸°ìˆ  íŠ¸ë Œë“œ ë¹…ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” **'í‚¤ì›Œë“œ ì¶”ì¶œ ì—”ì§„'**ì…ë‹ˆë‹¤.
ì•„ë˜ [ë°ì´í„°]ë¥¼ ì •ë°€ ë¶„ì„í•˜ì—¬, ê¸°ìˆ  íŠ¸ë Œë“œ ë¶„ì„ì— ìœ íš¨í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ **ìµœëŒ€í•œ ë§ì´(50ê°œ ì´ìƒ ëª©í‘œ)** ì¶”ì¶œí•˜ì„¸ìš”.

[ë°ì´í„° ì‹œì‘]
{doc_text}
[ë°ì´í„° ë]

# í•µì‹¬ ëª©í‘œ
1. **Quantity (ì–‘):** í…ìŠ¤íŠ¸ì— ë“±ì¥í•œ ìœ ì˜ë¯¸í•œ ê¸°ìˆ  í‚¤ì›Œë“œë¥¼ ë¹ ì§ì—†ì´ ê¸ì–´ëª¨ìœ¼ì„¸ìš”.
2. **Distinct (ë‹¤ì–‘ì„±):** ë˜‘ê°™ì€ ì˜ë¯¸ì˜ ë‹¨ì–´ë¥¼ ë°˜ë³µí•˜ì§€ ë§ê³ , **ì„œë¡œ ë‹¤ë¥¸ ì„¸ë¶€ ê¸°ìˆ **ì´ë‚˜ **êµ¬ì²´ì ì¸ ì£¼ì œ**ë¥¼ ì°¾ì•„ë‚´ì„¸ìš”.

# ì¶”ì¶œ ê·œì¹™ (Algorithm)
1. **[ë³µí•© ëª…ì‚¬êµ¬ ìš°ì„ ]**: ë‹¨ìˆœí•œ ë‹¨ì–´(ì˜ˆ: 'AI', 'ë¡œë´‡')ëŠ” ë„ˆë¬´ í¬ê´„ì ì…ë‹ˆë‹¤. ë³¸ë¬¸ì˜ ë§¥ë½ì„ ì‚´ë ¤ **'ì£¼ì²´ + ì„¸ë¶€ê¸°ìˆ /íŠ¹ì§•'** í˜•íƒœë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
   - (Bad): ì‚¼ì„±ì „ì, HBM, ë¡œë´‡, ìƒì„±í˜• AI
   - (Good): ì‚¼ì„±ì „ì HBM4, íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ ì œì–´, ìƒì„±í˜• AI í™˜ê° ë¬¸ì œ, ì˜¨ë””ë°”ì´ìŠ¤ AI ì¹©

2. **[ë³µí•© ëª…ì‚¬êµ¬ì™€ ë‹¨ìˆœ ëª…ì‚¬]**: ë³µí•©ëª…ì‚¬êµ¬ë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´, ê·¸ ì´í›„ì—ëŠ” í•´ë‹¹ ë³µí•©ëª…ì‚¬ ì•ˆì—ì„œì˜ ì¡°í•©ìœ¼ë¡œ ë¬¸ë§¥ì  ì˜ë¯¸ ì—†ì´ë„ ëª…í™•íŒ ê¸°ìˆ  í‚¤ì›Œë“œë¥¼ ë‹¤ìŒ í‚¤ì›Œë“œì— ì ì–´ë‘ì–´ì•¼í•©ë‹ˆë‹¤.
   ì˜ˆì‹œ: keyword
         AI ìƒì„± ì´ë¯¸ì§€ ì¸í˜ì¸íŒ… í‰ê°€,
         AI ìƒì„±,
         ì´ë¯¸ì§€ ìƒì„±,
         ì¸í˜ì¸íŒ…,
         AI í‰ê°€,

2. **[ì˜ë¯¸ì  ì¤‘ë³µ ì œê±° (Semantic De-duplication)]**: í‘œê¸°ë²•ë§Œ ë‹¤ë¥´ê³  ëœ»ì´ ê°™ì€ ë‹¨ì–´ëŠ” í•˜ë‚˜ë§Œ ë‚¨ê¸°ì„¸ìš”.
   - 'ì±—GPT'ì™€ 'ChatGPT'ê°€ ë‘˜ ë‹¤ ìˆë‹¤ë©´ -> ì˜ë¬¸ì¸ **'ChatGPT'**ë§Œ ì¶”ì¶œ
   - 'ì¸ê³µì§€ëŠ¥'ê³¼ 'AI'ê°€ ë‘˜ ë‹¤ ìˆë‹¤ë©´ -> ë” ì§§ê³  ëª…í™•í•œ **'AI'**ë§Œ ì¶”ì¶œ
   - ë‹¨, **'AI í•™ìŠµ'**ê³¼ **'AI ì¶”ë¡ '**ì€ ì„œë¡œ ë‹¤ë¥¸ ê¸°ìˆ  ë‹¨ê³„ì´ë¯€ë¡œ **ë‘˜ ë‹¤ ì¶”ì¶œ**í•´ì•¼ í•©ë‹ˆë‹¤. (ì´ ì°¨ì´ë¥¼ êµ¬ë¶„í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤)

3. **[ê¸ˆìœµ/ì¡ë‹´ í•„í„°ë§]**: ì•„ë˜ ë‚´ìš©ì€ ë°œê²¬ ì¦‰ì‹œ ì‚­ì œí•˜ì„¸ìš”.
   - ì£¼ê°€, ê¸‰ë“±/ê¸‰ë½, ëª©í‘œê°€, ì‹¤ì , ë°°ë‹¹, ì½”ì¸ ì‹œì„¸, íˆ¬ì ì „ë§
   - "ì¶©ê²©", "ëŒ€ë°•", "ê²°êµ­", "ê³µê°œ" ê°™ì€ ìˆ˜ì‹ì–´

4. **[ì—”í‹°í‹° ì •ê·œí™”]**:
   - ì˜ì–´ ê³ ìœ ëª…ì‚¬(ëª¨ë¸ëª…, ë¼ì´ë¸ŒëŸ¬ë¦¬ëª…)ëŠ” ì›ë¬¸ ì² ìë¥¼ ìœ ì§€í•˜ì„¸ìš”.

# ì¶œë ¥ ê°€ì´ë“œ
- ì¹´í…Œê³ ë¦¬ë¥¼ ë‚˜ëˆ„ì§€ ë§ê³ , ëª¨ë“  í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©í•˜ì„¸ìš”.
- ì¶œë ¥ì€ ë°˜ë“œì‹œ CSV í¬ë§·(í—¤ë” í¬í•¨)ìœ¼ë¡œ í•˜ì„¸ìš”.

keyword
ChatGPT-5o,
HBM4E ë©”ëª¨ë¦¬,
ì—”ë¹„ë””ì•„ ë¸”ë™ì›°,
ììœ¨ì£¼í–‰ ë ˆë²¨4,
Sora ì˜ìƒ ìƒì„±,
(ì´í•˜ ìƒëµ...)
    """
    
    print("   -> Geminiì—ê²Œ ë¶„ì„ ìš”ì²­ ì¤‘... (ì•½ 5~10ì´ˆ ì†Œìš”)")
    try:
        response = model.generate_content(prompt)
        result_text = response.text
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (```csv ... ```)
        result_text = re.sub(r'```csv', '', result_text)
        result_text = re.sub(r'```', '', result_text)
        
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±° (}}, ê³µë°± ë“±)
        result_text = re.sub(r'\}\}.*$', '', result_text, flags=re.MULTILINE)
        result_text = result_text.strip()
        
        # ë¹ˆ ì¤„ ì œê±°
        lines = [line.strip() for line in result_text.split('\n') if line.strip()]
        result_text = '\n'.join(lines)
        
        print("   -> ë¶„ì„ ì™„ë£Œ!")
        print(f"   -> ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¯¸ë¦¬ë³´ê¸°:")
        print("   " + "\n   ".join(lines[:5]))
        
        return result_text
        
    except Exception as e:
        print(f"   [Error] Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return None

# ==========================================
# ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    # 1. ë°ì´í„° ìˆ˜ì§‘
    docs = collect_data()
    
    # 2. Gemini ë¶„ì„
    csv_string = extract_keywords_with_gemini(docs)
    
    # 3. ê²°ê³¼ ì €ì¥
    if csv_string:
        try:
            # ë¬¸ìì—´ì„ StringIOë¡œ ë³€í™˜í•˜ì—¬ PDë¡œ ì½ê¸°
            from io import StringIO
            df = pd.read_csv(StringIO(csv_string))
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M')
            filename = f"gemini_trend_keywords_{timestamp}.csv"
            
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            print("\n" + "="*60)
            print(f"ğŸ‰ ë¶„ì„ ì„±ê³µ! íŒŒì¼ ì €ì¥ë¨: {filename}")
            print("="*60)
            print(df)
            
        except Exception as e:
            print(f"íŒŒì‹± ì—ëŸ¬: {e}")
            print("ì›ë³¸ ì‘ë‹µ:", csv_string)
    else:
        print("ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()