import google.generativeai as genai
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import pandas as pd
import time
import random
import os
import re
from datetime import datetime

# ==========================================
# [ì„¤ì •] API í‚¤ ë° íŒŒë¼ë¯¸í„°
# ==========================================
# êµ¬ê¸€ AI Studio(https://aistudio.google.com/)ì—ì„œ ë°œê¸‰ë°›ì€ í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”
GEMINI_API_KEY = "" 

# ìˆ˜ì§‘í•  í˜ì´ì§€ ìˆ˜ (DCì¸ì‚¬ì´ë“œ)
DC_PAGES = 2

# Gemini ëª¨ë¸ ì„¤ì •
genai.configure(api_key=GEMINI_API_KEY)
# ìµœì‹  íŠ¸ë Œë“œ ë¶„ì„ì—” ì†ë„ê°€ ë¹ ë¥¸ Flash ëª¨ë¸ì´ ì í•©í•©ë‹ˆë‹¤.
model = genai.GenerativeModel('gemini-2.5-flash')

# ==========================================
# 1. ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ
# ==========================================
def crawl_dcinside(pages=3):
    """DCì¸ì‚¬ì´ë“œ ì£¼ìš” ê°¤ëŸ¬ë¦¬ ê°œë…ê¸€ ìˆ˜ì§‘"""
    galleries = [
        ('programming', 'ê°œë°œì', 'major'),
        ('chatgpt', 'AI/ChatGPT', 'minor'),
        ('robot', 'ë¡œë´‡', 'minor')
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://www.dcinside.com/'
    }
    
    titles = []
    base_url = "https://gall.dcinside.com"
    
    # ë…¸ì´ì¦ˆ í•„í„°ë§ íŒ¨í„´
    noise_patterns = ['ã…‹', 'ã…', 'ã„·', 'ã…‡ã…‡', '...', '??', '!!']
    
    print(f"   -> DCì¸ì‚¬ì´ë“œ {len(galleries)}ê°œ ê°¤ëŸ¬ë¦¬ ìˆœíšŒ ì¤‘...")
    
    for g_id, g_name, g_type in galleries:
        for page in range(1, pages + 1):
            try:
                if g_type == 'major':
                    url = f"{base_url}/board/lists/?id={g_id}&exception_mode=recommend&page={page}"
                else:
                    url = f"{base_url}/mgallery/board/lists/?id={g_id}&exception_mode=recommend&page={page}"
                
                res = requests.get(url, headers=headers, timeout=5)
                if res.status_code != 200: continue
                
                soup = BeautifulSoup(res.text, 'html.parser')
                rows = soup.select('.ub-content.us-post')
                if not rows: rows = soup.select('tr.ub-content')
                
                for row in rows:
                    try:
                        title_tag = row.select_one('.gall_tit a')
                        if title_tag:
                            title = title_tag.text.strip()
                            # ê°„ë‹¨í•œ ì „ì²˜ë¦¬
                            if len(title) < 2: continue
                            if any(p * 3 in title for p in noise_patterns): continue # ã…‹ã…‹ã…‹ ë°˜ë³µ ë“±
                            titles.append(f"[{g_name}] {title}") # ê°¤ëŸ¬ë¦¬ ì´ë¦„ í¬í•¨í•´ì„œ ë¬¸ë§¥ ì œê³µ
                    except: pass
                
                time.sleep(random.uniform(0.5, 1.0)) # ì°¨ë‹¨ ë°©ì§€
            except Exception as e:
                print(f"      [Error] {g_name}: {e}")
                
    return list(set(titles))

def collect_data():
    """RSS ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘ (Selenium ë¶ˆí•„ìš”)"""
    print("\n" + "="*60)
    print("ğŸ“¡ [Phase 1] ë°ì´í„° ìˆ˜ì§‘ (ê¸°ìˆ  ë¸”ë¡œê·¸ RSS)")
    print("="*60)
    
    raw_data = []
    headers = {"User-Agent": "Mozilla/5.0"}
    
    # ë‰´ìŠ¤ RSS
    news_rss = [
        ('ê¸±ë‰´ìŠ¤', 'https://feeds.feedburner.com/geeknews-feed'),
    ]
    
    # í•œêµ­ IT ê¸°ì—… ê¸°ìˆ  ë¸”ë¡œê·¸ RSS
    tech_blog_rss = [
        ('ë¬´ì‹ ì‚¬', 'https://medium.com/feed/musinsa-tech'),
        ('ë„¤ì´ë²„ D2', 'https://d2.naver.com/d2.atom'),
        ('ë§ˆì¼“ì»¬ë¦¬', 'https://helloworld.kurly.com/feed.xml'),
        ('ìš°ì•„í•œí˜•ì œë“¤', 'https://techblog.woowahan.com/feed'),
        ('ì¹´ì¹´ì˜¤ì—”í„°', 'https://tech.kakaoenterprise.com/feed'),
        ('ë°ë¸Œì‹œìŠ¤í„°ì¦ˆ', 'https://tech.devsisters.com/rss.xml'),
        ('ë¼ì¸', 'https://engineering.linecorp.com/ko/feed/index.html'),
        ('ì¿ íŒ¡', 'https://medium.com/feed/coupang-engineering'),
        ('ë‹¹ê·¼ë§ˆì¼“', 'https://medium.com/feed/daangn'),
        ('í† ìŠ¤', 'https://toss.tech/rss.xml'),
        ('ì§ë°©', 'https://medium.com/feed/zigbang'),
        ('ì™“ì± ', 'https://medium.com/feed/watcha'),
        ('ë±…í¬ìƒëŸ¬ë“œ', 'https://blog.banksalad.com/rss.xml'),
        ('Hyperconnect', 'https://hyperconnect.github.io/feed.xml'),
        ('ìš”ê¸°ìš”', 'https://techblog.yogiyo.co.kr/feed'),
        ('ì˜ì¹´', 'https://tech.socarcorp.kr/feed'),
        ('ë¦¬ë””', 'https://www.ridicorp.com/feed'),
        ('NHN Toast', 'https://meetup.toast.com/rss'),
        ('Velog', 'https://v2.velog.io/rss/'),
        ('ê°œë°œììŠ¤ëŸ½ë‹¤', 'https://blog.gaerae.com/feeds/posts/default?alt=rss'),
        ('44BITS', 'https://www.44bits.io/ko/feed/all'),
    ]
    
    all_rss = news_rss + tech_blog_rss
    
    print("   -> RSS ìˆ˜ì§‘ ì¤‘...")
    for name, url in all_rss:
        try:
            res = requests.get(url, headers=headers, timeout=10)
            root = ET.fromstring(res.content)
            count = 0
            
            # Atom í˜•ì‹
            if any(x in url for x in ['geeknews', 'd2.naver', 'hyperconnect']):
                for item in root.findall(".//{http://www.w3.org/2005/Atom}entry"):
                    try:
                        title_elem = item.find("{http://www.w3.org/2005/Atom}title")
                        if title_elem is not None and title_elem.text:
                            raw_data.append(f"[{name}] {title_elem.text}")
                            count += 1
                    except: pass
            
            # RSS 2.0 í˜•ì‹
            if count == 0:
                for item in root.findall(".//item"):
                    try:
                        title_elem = item.find("title")
                        if title_elem is not None and title_elem.text:
                            raw_data.append(f"[{name}] {title_elem.text}")
                            count += 1
                    except: pass
            
            if count > 0:
                print(f"      -> {name}: {count}ê°œ")
        except:
            pass
    
    print(f"   -> ì´ {len(raw_data)}ê°œ í…ìŠ¤íŠ¸ í™•ë³´.\n")
    return raw_data

# ==========================================
# 2. Gemini ë¶„ì„ ëª¨ë“ˆ (BERT ëŒ€ì²´)
# ==========================================
def extract_keywords_with_gemini(docs):
    print("\n" + "="*60)
    print(f"ğŸ§  [Phase 2] Gemini 1.5 Flash íŠ¸ë Œë“œ ë¶„ì„")
    print("="*60)
    
    if not docs: return []
    
    # ë„ˆë¬´ ë§ì€ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë³´ë‚´ë©´ í† í° ì œí•œì— ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒ˜í”Œë§í•˜ê±°ë‚˜ ë°°ì¹˜ë¥¼ ë‚˜ëˆ”
    # ì—¬ê¸°ì„œëŠ” ìµœì‹  ë°ì´í„° ìœ„ì£¼ë¡œ ìµœëŒ€ 500ê°œë§Œ ì¶”ë ¤ì„œ ë³´ëƒ„ (Flash ëª¨ë¸ì€ 100ë§Œ í† í°ê¹Œì§€ ê°€ëŠ¥í•˜ê¸´ í•¨)
    docs_sample = docs[:2000] 
    
    doc_text = "\n".join(docs_sample)
    
    # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ (í•µì‹¬) - reason ì»¬ëŸ¼ ì œê±°í•˜ì—¬ íŒŒì‹± ì—ëŸ¬ ë°©ì§€
    prompt = f"""
    ì•„ë˜ í…ìŠ¤íŠ¸ëŠ” í˜„ì¬ í•œêµ­ì˜ ê¸°ìˆ  ë‰´ìŠ¤ í—¤ë“œë¼ì¸ê³¼ ê¸°ìˆ ë¸”ë¡œê·¸ ê²Œì‹œê¸€ì˜ ì œëª©ë“¤ì…ë‹ˆë‹¤.

    [ë°ì´í„° ì‹œì‘]
    {doc_text}
    [ë°ì´í„° ë]

    [ëª©í‘œ]
    ì´ í…ìŠ¤íŠ¸ë“¤ì„ ë¶„ì„í•˜ì—¬, AI/IT ê¸°ìˆ  íŠ¸ë Œë“œì™€ ê´€ë ¨ëœ í‚¤ì›Œë“œ 30ê°œ ì´ìƒì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

    [ë§¤ìš° ì¤‘ìš”]
    - í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ [ë°ì´í„° ì‹œì‘]ê³¼ [ë°ì´í„° ë] ì‚¬ì´ í…ìŠ¤íŠ¸ì— ì‹¤ì œë¡œ ë“±ì¥í•œ ë‹¨ì–´Â·í‘œí˜„ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
    - ë‰´ìŠ¤/ì»¤ë®¤ë‹ˆí‹°ì— ë“±ì¥í•˜ì§€ ì•Šì€ ê¸°ìˆ ëª…, íšŒì‚¬ëª…, ëª¨ë¸ëª…, ì•½ì–´ë¥¼ ìƒˆë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
    - ì˜ì–´ ê³ ìœ ëª…ì‚¬ì™€ ëª¨ë¸ëª…(ChatGPT, GPT-5.1, Gemini ë“±)ì€ ì›ë¬¸ì— ë‚˜ì˜¨ ì² ìë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.

    [ì œì™¸í•  ì¹´í…Œê³ ë¦¬]
    - ì£¼ì‹, íˆ¬ì, ì½”ì¸, ì•”í˜¸í™”í, ê¸ˆë¦¬, í™˜ìœ¨ ë“± ê¸ˆìœµ/íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œëŠ” ì™„ì „íˆ ì œì™¸í•˜ì„¸ìš”.
    - íŠ¹ì • ê¸°ì—…ì˜ ì£¼ê°€ë‚˜ ì¬ë¬´ ê´€ë ¨ ë‚´ìš©ë„ ì œì™¸í•˜ì„¸ìš”.

    [ì§‘ì¤‘í•  ì¹´í…Œê³ ë¦¬]
    - AI/ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ  (ChatGPT, Gemini, LLM, ë”¥ëŸ¬ë‹ ë“±)
    - ë¡œë´‡/ìë™í™” ê¸°ìˆ  (íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡, ì‚°ì—…ìš© ë¡œë´‡ ë“±)
    - ë°˜ë„ì²´/í•˜ë“œì›¨ì–´ ê¸°ìˆ  (AI ì¹©, GPU, HBM ë“±ì˜ 'ê¸°ìˆ ' ìì²´)
    - ì†Œí”„íŠ¸ì›¨ì–´/í”Œë«í¼ (ì•±, ì„œë¹„ìŠ¤, í”„ë ˆì„ì›Œí¬ ë“±)
    - ì‹ ê¸°ìˆ /ë¯¸ë˜ê¸°ìˆ  (ì–‘ìì»´í“¨íŒ…, AR/VR, ë©”íƒ€ë²„ìŠ¤ ë“±)
    - ë°ì´í„°/í´ë¼ìš°ë“œ ê¸°ìˆ 

    [ì¡°ê±´]
    1. ì¶”ìƒì ì¸ ë‹¨ì–´(ìƒìŠ¹, ì „ë§, í­ë½, íŠ¹ì§•ì£¼, ë§ˆê°, ì½”ìŠ¤í”¼)ëŠ” ì ˆëŒ€ ì œì™¸í•˜ì„¸ìš”.
    2. ë‹¨ì–´ ìì²´ê°€ ì£¼ëŠ” ì˜ë¯¸ê°€ ì—¬ëŸ¬ ê°€ì§€ì¼ ê²½ìš° ë§¥ë½ì„ í¬í•¨í•˜ëŠ” 'ì£¼ì²´ + ì‚¬ê±´/ì¬ë£Œ' í˜•íƒœì˜ ë³µí•© í‚¤ì›Œë“œë¡œ ë§Œë“œì„¸ìš”.
    - ë‚˜ìœ ì˜ˆ: ì‚¼ì„±ì „ì, ì—”ë¹„ë””ì•„, ë¡œë´‡, ê¸ˆë¦¬
    - ì¢‹ì€ ì˜ˆ: ì‚¼ì„±ì „ì HBM, ì—”ë¹„ë””ì•„ AIì¹©, íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡, GPT ê²€ì—´ ë…¼ë€
    3. ë³µí•© í‚¤ì›Œë“œë„ ì›ë¬¸ì— ë“±ì¥í•œ ë‹¨ì–´ë§Œ ì¡°í•©í•´ì„œ ë§Œë“œì„¸ìš”.
    - ì˜ˆ: "ì‚¼ì„±ì „ì HBM"ì´ ì›ë¬¸ì— ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥, "ì‚¼ì„±ì „ì HBM ê¸°ìˆ "ì²˜ëŸ¼ 'ê¸°ìˆ ' í•œ ë‹¨ì–´ë§Œ ì¶”ê°€í•˜ëŠ” ê²ƒì€ í—ˆìš©
    4. ë³µí•©í‚¤ì›Œë“œë¼ë„ ì§€ë‚˜ì¹˜ê²Œ ë³µì¡í•´ì§€ê±°ë‚˜ ê¸¸ì–´ì§€ëŠ”ê²ƒì„ ê²½ê³„í•˜ì„¸ìš”
    5. ì»¤ë®¤ë‹ˆí‹° ì€ì–´(ë”í™©ì± , ë–¡ìƒ ë“±)ëŠ” ê·¸ ì›ì¸ì´ ë˜ëŠ” í‘œì¤€ì–´ ì´ìŠˆë¡œ ë²ˆì—­í•˜ì„¸ìš”.
    6. ê¸°ìˆ  ìì²´ì— ì´ˆì ì„ ë§ì¶”ì„¸ìš”.
    - "ì—”ë¹„ë””ì•„ ì£¼ê°€" â†’ ì œì™¸
    - "ì—”ë¹„ë””ì•„ AIì¹© ì„±ëŠ¥" â†’ í¬í•¨
    - "ì‚¼ì„±ì „ì ì‹¤ì " â†’ ì œì™¸
    - "ì‚¼ì„±ì „ì HBM ê¸°ìˆ " â†’ í¬í•¨
    7. ë°˜ë“œì‹œ 30ê°œ ì´ìƒ ì±„ì›Œì£¼ì„¸ìš”. ë§ì„ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤.
    8. ì¤„ì„ë§ì´ë‚˜ ì€ì–´ê°€ ìˆì„ ê²½ìš°, ì›ë¬¸ì— ë“±ì¥í•œ ê³µì‹ ëª…ì¹­(íšŒì‚¬/ì„œë¹„ìŠ¤/ëª¨ë¸ ì´ë¦„)ìœ¼ë¡œ ë³€í™˜í•´ í‚¤ì›Œë“œë¥¼ ë§Œë“œì„¸ìš”.

    [ì¶œë ¥ í˜•ì‹]
    ë°˜ë“œì‹œ ì•„ë˜ CSV í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. (í—¤ë” í¬í•¨, keyword ì•ˆì—ëŠ” ì‰¼í‘œë¥¼ ë„£ì§€ ë§ˆì„¸ìš”)
    categoryëŠ” AI, ë¡œë´‡, ë°˜ë„ì²´, ì†Œí”„íŠ¸ì›¨ì–´, ë°ì´í„°, í´ë¼ìš°ë“œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

    keyword,category
    ChatGPT ì„±ëŠ¥ í–¥ìƒ,AI
    íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ ê¸°ìˆ ,ë¡œë´‡
    HBM ë©”ëª¨ë¦¬ ê¸°ìˆ ,ë°˜ë„ì²´
    """
    
    print("   -> Geminiì—ê²Œ ë¶„ì„ ìš”ì²­ ì¤‘... (ì•½ 5~10ì´ˆ ì†Œìš”)")
    try:
        response = model.generate_content(prompt)
        result_text = response.text
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (```csv ... ```)
        result_text = re.sub(r'```csv', '', result_text)
        result_text = re.sub(r'```', '', result_text)
        
        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±° (}}, ê³µë°± ë“±)
        result_text = re.sub(r'\}\}.*$', '', result_text, flags=re.MULTILINE)
        result_text = result_text.strip()
        
        # ë¹ˆ ì¤„ ì œê±°
        lines = [line.strip() for line in result_text.split('\n') if line.strip()]
        result_text = '\n'.join(lines)
        
        print("   -> ë¶„ì„ ì™„ë£Œ!")
        print(f"   -> ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¯¸ë¦¬ë³´ê¸°:")
        print("   " + "\n   ".join(lines[:5]))
        
        return result_text
        
    except Exception as e:
        print(f"   [Error] Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return None

# ==========================================
# ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    # 1. ë°ì´í„° ìˆ˜ì§‘
    docs = collect_data()
    
    # 2. Gemini ë¶„ì„
    csv_string = extract_keywords_with_gemini(docs)
    
    # 3. ê²°ê³¼ ì €ì¥
    if csv_string:
        try:
            # ë¬¸ìì—´ì„ StringIOë¡œ ë³€í™˜í•˜ì—¬ PDë¡œ ì½ê¸°
            from io import StringIO
            df = pd.read_csv(StringIO(csv_string))
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M')
            filename = f"gemini_trend_keywords_{timestamp}.csv"
            
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            print("\n" + "="*60)
            print(f"ğŸ‰ ë¶„ì„ ì„±ê³µ! íŒŒì¼ ì €ì¥ë¨: {filename}")
            print("="*60)
            print(df)
            
        except Exception as e:
            print(f"íŒŒì‹± ì—ëŸ¬: {e}")
            print("ì›ë³¸ ì‘ë‹µ:", csv_string)
    else:
        print("ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()