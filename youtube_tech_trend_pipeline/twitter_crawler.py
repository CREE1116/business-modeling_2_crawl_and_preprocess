import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
import pandas as pd
import time
import random
import pickle
import os
import urllib.parse
import re
from datetime import datetime

# ==========================================
# [ì„¤ì •] ìˆ˜ì§‘ ì˜µì…˜ (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”)
# ==========================================
CSV_FILE_PATH = "gemini_trend_keywords_20251211_2350.csv" 
COOKIE_FILE = "twitter_cookies.pkl"

TWEETS_PER_QUERY_GROUP = 200  # ì¿¼ë¦¬ ì„¸íŠ¸ ë‹¹ ìˆ˜ì§‘ ëª©í‘œ
SEARCH_MODE = "live"          # live: ìµœì‹ ìˆœ (ì–‘ í™•ë³´ìš©), top: ì¸ê¸°ìˆœ
LANG_FILTER = "lang:ko"       

# [1] ë¦¬íŠ¸ìœ— ì»·ì˜¤í”„ ì„¤ì • (1ì°¨ í•„í„°ë§)
# íŠ¸ìœ—ì´ ê²€ìƒ‰ë  ë•Œ ìµœì†Œ ì´ ìˆ«ì ì´ìƒì˜ ë¦¬íŠ¸ìœ—ì´ ìˆì–´ì•¼ë§Œ ë‚˜ì˜µë‹ˆë‹¤.
# 0ì´ë©´ ëª¨ë“  ê¸€, 5~10 ì •ë„ë©´ ì ë‹¹í•œ í€„ë¦¬í‹°, 50 ì´ìƒì´ë©´ ë„¤ì„ë“œ ê¸€ë§Œ ë‚˜ì˜´.
MIN_RETWEETS = 1

# [2] ë‚ ì§œ ë²”ìœ„ ì„¤ì •
# ìµœê·¼ ë©°ì¹  ë™ì•ˆì˜ íŠ¸ìœ—ë§Œ ê²€ìƒ‰ (ì˜ˆ: 7 = ìµœê·¼ 7ì¼, 30 = ìµœê·¼ 1ê°œì›”)
SEARCH_DAYS = 365  
# [ë‚´ë¶€ ê²€ë¬¸] íŒŒì´ì¬ìœ¼ë¡œ ê±¸ëŸ¬ë‚¼ ì „ì²´ ë…¸ì´ì¦ˆ ë¦¬ìŠ¤íŠ¸
# 1. ì• ë‹ˆë©”ì´ì…˜/ì˜¤íƒ€ì¿  ê´€ë ¨
anime_otaku = [
    'ì• ë‹ˆ', 'ì• ë‹ˆë©”ì´ì…˜', 'ì˜¤íƒ€ì¿ ', 'ì½”ìŠ¤í”„ë ˆ', 'ì½”ìŠ¤ì–´', 'ë§Œí™”',
    'ë¼ì´íŠ¸ë…¸ë²¨', 'ë¼ë…¸ë²¨', 'ì„±ìš°', 'ë•ì§ˆ', 'ë•í›„', 'êµ¿ì¦ˆ',
    'í”¼ê·œì–´', 'ì•„í¬ë¦´', 'ìŠ¤íƒ ë“œ', 'ëŸ¬ë²„ìŠ¤íŠ¸ë©', 'ì¼ëŸ¬ìŠ¤íŠ¸', 'ë™ì¸',
    'ì½”ë¯¹ë§ˆì¼“', 'ì½”ë¯¸ì¼€', '2D', 'ì™íƒ€ë²„ìŠ¤', 'ë²„ì¸„ì–¼', 'ë²„íŠœë²„',
    'í™€ë¡œë¼ì´ë¸Œ', 'ë‹ˆì§€ì‚°ì§€', 'ì•„ì´ëŒë§ˆìŠ¤í„°', 'ëŸ¬ë¸Œë¼ì´ë¸Œ', 'ì›ì‹ ',
    'ë¶•ê´´', 'ëª…ì¼ë°©ì£¼', 'ë¸”ë£¨ì•„ì¹´', 'ë¸”ì•„', 'ìš°ë§ˆë¬´ìŠ¤ë©”'
]

# 2. ì•„ì´ëŒ/ì—°ì˜ˆì¸ ê´€ë ¨
idol_celeb = [
    'ì•„ì´ëŒ', 'ê±¸ê·¸ë£¹', 'ë³´ì´ê·¸ë£¹', 'ì»´ë°±', 'ìŒë°©', 'ë®¤ì§ë±…í¬', 'ìŒì¤‘',
    'íŒ¬ë¤', 'íŒ¬ì‹¸', 'íŒ¬ë¯¸íŒ…', 'ì½˜ì„œíŠ¸', 'ì§ìº ', 'ë¬´ëŒ€', 'ì‡¼ì¼€ì´ìŠ¤',
    'ë°ë·”', 'ì†”ë¡œ', 'ìœ ë‹›', 'ì„¼í„°', 'ë¹„ì£¼ì–¼', 'ë©”ë³´', 'ë¦¬ë³´',
    'í¬ì¹´', 'ì•¨ë²”', 'ì´ˆë™', '1ìœ„', 'ì°¨íŠ¸', 'ìŠ¤ë°', 'ë©œë¡ ', 'ì§€ë‹ˆ',
    'BTS', 'ë¸”ë™í•‘í¬', 'ë‰´ì§„ìŠ¤', 'ì•„ì´ë¸Œ', 'ì—ìŠ¤íŒŒ', 'ë¥´ì„¸ë¼í•Œ',
    'ì—”ì‹œí‹°', 'ì„¸ë¸í‹´', 'íˆ¬ëª¨ë¡œìš°ë°”ì´íˆ¬ê²Œë”', 'ìˆì§€', 'ìŠ¤í…Œì´ì”¨',
    'íœ', 'ë•ì§ˆ', 'ìµœì• ', 'ë³¸ì§„', 'ì…ë•', 'íƒˆë•'
]

# 3. ìŠ¤í¬ì¸ (ì•¼êµ¬/ì¶•êµ¬ ë“±) ê´€ë ¨
sports = [
    'ì•¼êµ¬', 'í”„ë¡œì•¼êµ¬', 'KBO', 'MLB', 'ë¡¯ë°', 'ë‘ì‚°', 'LG', 'ì‚¼ì„±', 
    'í•œí™”', 'SSG', 'KT', 'NC', 'í‚¤ì›€', 'KIA',
    'íƒ€ì', 'íˆ¬ìˆ˜', 'í™ˆëŸ°', 'ì•ˆíƒ€', 'ì„ ë°œ', 'ë¶ˆíœ', 'ë§ˆë¬´ë¦¬',
    'ê°ë…', 'ì½”ì¹˜', 'ì„ ìˆ˜', 'íŠ¸ë ˆì´ë“œ', 'FA', 'ê³„ì•½', 'ì—°ë´‰',
    'ê²½ê¸°', 'ì´ë‹', 'ë“ì ', 'ì‹¤ì ', 'ìŠ¹ë¦¬', 'íŒ¨ë°°', 'ë¬´ìŠ¹ë¶€',
    'ì¶•êµ¬', 'EPL', 'í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸', 'ë¼ë¦¬ê°€', 'ë¶„ë°ìŠ¤ë¦¬ê°€', 'ì„¸ë¦¬ì—',
    'ì†í¥ë¯¼', 'ì´ê°•ì¸', 'ê¹€ë¯¼ì¬', 'í™©í¬ì°¬', 'í† íŠ¸ë„˜', 'ë§¨ìœ ', 'ë§¨ì‹œí‹°',
    'ìŠ¹ë¶€ì¡°ì‘', 'ìŠ¤í¬ì¸ í† í† ', 'ë² íŒ…'
]

# 4. ëŒ€ì¶œ/ê¸ˆìœµì‚¬ê¸° ê´€ë ¨
loan_finance = [
    'ëŒ€ì¶œ', 'ì†Œì•¡ëŒ€ì¶œ', 'ë¬´ì§ìëŒ€ì¶œ', 'ì‹ ìš©ëŒ€ì¶œ', 'ë‹´ë³´ëŒ€ì¶œ', 'ê¸‰ì „',
    'ê°œì¸ëˆ', 'ì‚¬ì±„', 'ë¬´ë°©ë¬¸', 'ë¬´ì„œë¥˜', 'ë‹¹ì¼ëŒ€ì¶œ', 'ì¦‰ì‹œëŒ€ì¶œ',
    'ì €ê¸ˆë¦¬', 'ê³ ì•¡ëŒ€ì¶œ', 'í•œë„ì¡°íšŒ', 'ì‹ ìš©íšŒë³µ', 'ì±„ë¬´í†µí•©',
    'ì—°ì²´ì', 'ì‹ ë¶ˆì', 'íŒŒì‚°', 'íšŒìƒ', 'ë©´ì±…',
    'í–‡ì‚´ë¡ ', 'ìƒˆí¬ë§í™€ì”¨', 'ì§ì¥ì¸ëŒ€ì¶œ', 'í”„ë¦¬ëœì„œëŒ€ì¶œ',
    'ë¹„ëŒ€ë©´', 'ëª¨ë°”ì¼ëŒ€ì¶œ', 'P2P', 'í•€í…Œí¬', 'ì €ì¶•ì€í–‰',
    'ëŒ€ë¶€ì—…', 'ì¤‘ê°œ', 'ìƒë‹´', 'ë¬¸ì˜', 'DM', 'ì¹´í†¡', 'í…”ë ˆê·¸ë¨'
]

# 5. ë„ë°•/ë¶ˆë²•ë² íŒ… ê´€ë ¨
gambling = [
    'í† í† ', 'ìŠ¤í¬ì¸ í† í† ', 'ë°°íŒ…', 'ë² íŒ…', 'ì¹´ì§€ë…¸', 'ë°”ì¹´ë¼', 'ìŠ¬ë¡¯',
    'ë¨¹íŠ€', 'ì‚¬ì„¤í† í† ', 'ì‚¬ì„¤', 'ì•ˆì „ë†€ì´í„°', 'ë©”ì´ì €ì‚¬ì´íŠ¸',
    'ë¼ì´ë¸Œì¹´ì§€ë…¸', 'ì˜¨ë¼ì¸ì¹´ì§€ë…¸', 'í•´ì™¸ë°°íŒ…', 'í•´ì™¸ì‚¬ì´íŠ¸',
    'ë³´ì¦ì—…ì²´', 'ê½ë¨¸ë‹ˆ', 'í™˜ì „', 'ì¶©ì „', 'ì…ê¸ˆ', 'ì¶œê¸ˆ',
    'í”½ìŠ¤í„°', 'ìŠ¤í¬ì¸ ë¶„ì„', 'ìŠ¹ë¶€ì˜ˆì¸¡', 'í•´ì™¸ì¶•êµ¬', 'EPLë² íŒ…',
    'ì—ë³¼ë£¨ì…˜', 'í”„ë¼ê·¸ë§ˆí‹±', 'ë§ˆì´í¬ë¡œê²Œì´ë°', 'í…”ë ˆê·¸ë¨', 'VIP'
]

# 6. ì„±ì¸/ë¶ˆê±´ì „ ê´€ë ¨
adult_content = [
    'ë–¡ë°©', 'ë–¡', '19', 'ì„±ì¸', 'ì•¼ë™', 'ì•¼ì‚¬', 'ì—ë¡œ',
    'ì¡°ê±´', 'ì›ì¡°', 'ë§Œë‚¨', 'ì„¹íŒŒ', 'í°íŒ…', 'ì±„íŒ…', 'í™”ìƒ',
    'ì˜¤í”„', 'ì§ê±°ë˜', 'í›„ë¶ˆì œ', 'ì„ ë¶ˆì œ', 'í˜ì´', 'í›„ê¸°',
    'í…”ë ˆë°©', 'í…”ë°©', 'ì˜¤í”ˆë°©', 'në²ˆë°©', 'ëª¸ìº ', 'ì˜í†µ',
    'ìˆ ì§‘', 'ìœ í¥', 'ë£¸ì‚´ë¡±', 'ë…¸ë˜ë°©', 'ì•ˆë§ˆ', 'ë§ˆì‚¬ì§€'
]

# 7. ì‡¼í•‘ëª°/ë§ˆì¼€íŒ… ìŠ¤íŒ¸
shopping_spam = [
    'ìµœì €ê°€', 'ë¬´ë£Œë°°ì†¡', 'í• ì¸', 'ì¿ í°', 'ì ë¦½', 'ì´ë²¤íŠ¸',
    'ë‹¹ì²¨', 'ê²½í’ˆ', 'ì¶”ì²¨', 'ë¬´ë£Œë‚˜ëˆ”', 'ì„ ì°©ìˆœ',
    'êµ¬ë§¤ë§í¬', 'ì‡¼í•‘ëª°', 'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´', 'ì˜¤í”ˆë§ˆì¼“',
    'ì¿ íŒ¡', 'ì•Œë¦¬', 'íƒ€ì˜¤ë°”ì˜¤', 'ì§êµ¬', 'êµ¬ë§¤ëŒ€í–‰',
    'íŒ”ë¡œìš°', 'ì¢‹ì•„ìš”', 'ë¦¬íŠ¸ìœ—', 'RT', 'ë©˜ì…˜',
    'í™ë³´', 'ê´‘ê³ ', 'ë§ˆì¼€íŒ…', 'ì œíœ´', 'í˜‘ì°¬'
]

# 8. ì‚¬ê¸°/í”¼ì‹± ê´€ë ¨
scam_phishing = [
    'ë‹¹ì²¨ê¸ˆ', 'í™˜ê¸‰', 'ì„¸ê¸ˆí™˜ê¸‰', 'ë¯¸ìˆ˜ë ¹', 'ì¡°íšŒ',
    'ë¬´ë£Œì§€ê¸‰', 'ë³´ìƒê¸ˆ', 'í¬ì¸íŠ¸', 'ë§ˆì¼ë¦¬ì§€', 'ì ë¦½ê¸ˆ',
    'ë³¸ì¸ì¸ì¦', 'ì‹¤ëª…ì¸ì¦', 'ê³„ì¢Œì¸ì¦', 'ì¹´ë“œë“±ë¡',
    'í´ë¦­', 'ë§í¬', 'ì ‘ì†', 'ë°”ë¡œê°€ê¸°', 'URL',
    'ê¸´ê¸‰', 'ì¦‰ì‹œ', 'ë¹ ë¥¸', 'ì‹ ì†', 'ë‹¹ì¼',
    'ì •ë¶€ì§€ì›', 'ì •ë¶€í˜œíƒ', 'êµ­ê°€ì§€ì›', 'ì½”ë¡œë‚˜ì§€ì›ê¸ˆ'
]

# 9. ê°€ìƒí™”í ì‚¬ê¸°
crypto_scam = [
    'ì½”ì¸', 'ë¹„íŠ¸ì½”ì¸', 'ì´ë”ë¦¬ì›€', 'ë¦¬í”Œ', 'ì•ŒíŠ¸ì½”ì¸',
    'ìƒì¥', 'ì—ì–´ë“œë', 'ì—ì–´ë“œë¡­', 'í”„ë¦¬ì„¸ì¼', 'ICO', 'IDO',
    'íŒí•‘', 'í­ë“±', 'ê¸‰ë“±', 'ëŒ€ë°•', 'ìˆ˜ìµì¸ì¦',
    'ì‹œê·¸ë„', 'ì„ ë¬¼ê±°ë˜', 'ë ˆë²„ë¦¬ì§€', 'ë§ˆì§„ê±°ë˜',
    'ë‹¨í†¡ë°©', 'ì˜¤í”ˆì±„íŒ…', 'í…”ë ˆê·¸ë¨ë°©', 'ë””ìŠ¤ì½”ë“œ',
    'NFT', 'ë©”íƒ€ë²„ìŠ¤', 'P2E', 'ê²Œì„íŒŒì´'
]

# 10. ì •ì¹˜/í˜ì˜¤ ê´€ë ¨ (ì„ íƒì )
political_hate = [
    'ì¢ŒíŒŒ', 'ìš°íŒŒ', 'ì¢…ë¶', 'ë¹¨ê°±ì´', 'ìˆ˜ê¼´', 'ë§¤êµ­ë…¸',
    'í˜ë¯¸', 'í•œë‚¨', 'ëœì¥ë…€', 'ê¹€ì¹˜ë…€', 'ë§˜ì¶©', 'í‹€ë”±',
    'ê¸‰ì‹ì¶©', 'ë…¸ë¬´í˜„', 'ë°•ê·¼í˜œ', 'ë¬¸ì¬ì¸', 'ìœ¤ì„ì—´',
    'ë¯¼ì£¼ë‹¹', 'êµ­ë¯¼ì˜í˜', 'ì •ì˜ë‹¹', 'êµ­ì§', 'ë”ë¶ˆì–´',
    'ì¼ë² ', 'ë©”ê°ˆ', 'ì›Œë§ˆë“œ', 'ë””ì‹œ', 'í¨ì½”'
]

# ì „ì²´ í•„í„°ë§ ë‹¨ì–´ í†µí•©
all_spam_keywords = (
    # anime_otaku + idol_celeb + sports + 
    # loan_finance + gambling + adult_content +
    # shopping_spam + scam_phishing + crypto_scam +
    # political_hate
)

# ì¤‘ë³µ ì œê±°
ALL_NOISE_KEYWORDS = list(set(all_spam_keywords))

# [ì…êµ¬ ì»·] ì¿¼ë¦¬ì— ì§ì ‘ ë„£ì„ ì œì™¸ì–´ (ë…ë¦½ ê²€ìƒ‰ì´ë¯€ë¡œ ê°•í•˜ê²Œ ì„¤ì • ê°€ëŠ¥)
QUERY_EXCLUDE_KEYWORDS = [

]

# ==========================================
# 1. ìœ í‹¸ë¦¬í‹°
# ==========================================
def random_sleep(min_t=2.0, max_t=4.0):
    time.sleep(random.uniform(min_t, max_t))

# ==========================================
# [ìˆ˜ì •] ìŠ¤í¬ë¡¤ ë¡œì§ ê°œì„  (ìš”ìš” ìŠ¤í¬ë¡¤ ì ìš©)
# ==========================================
def smart_scroll(driver, last_height, stuck_count):
    """
    ë‹¨ìˆœ ìŠ¤í¬ë¡¤ì´ ì•„ë‹ˆë¼, ë°”ë‹¥ì„ ì°ê³  ì‚´ì§ ì˜¬ë ¸ë‹¤ê°€ ë‹¤ì‹œ ë‚´ë¦¬ëŠ” 
    'ìš”ìš” ë™ì‘'ì„ í†µí•´ íŠ¸ìœ„í„°ì˜ ë°ì´í„° ë¡œë”©ì„ ê°•ì œë¡œ íŠ¸ë¦¬ê±°í•¨
    """
    try:
        # 1. í˜„ì¬ ë†’ì´ì—ì„œ í™”ë©´ ì ˆë°˜ ì •ë„ ë‚´ë¦¼ (ìì—°ìŠ¤ëŸ½ê²Œ)
        driver.execute_script("window.scrollBy(0, window.innerHeight * 0.8);")
        time.sleep(random.uniform(1.0, 1.5))

        # 2. ë°”ë‹¥ê¹Œì§€ í™• ë‚´ë¦¼
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(2.0, 3.0))

        # 3. [í•µì‹¬] ë§Œì•½ ë†’ì´ ë³€í™”ê°€ ì—†ì–´ì„œ 'stuck' ìƒíƒœë¼ë©´? -> ì¶©ê²© ìš”ë²•
        if stuck_count > 0:
            # 3-1. ìœ„ë¡œ ì‚´ì§ ì˜¬ë¦¼ (ë¡œë”© íŠ¸ë¦¬ê±°)
            driver.execute_script("window.scrollBy(0, -500);")
            time.sleep(random.uniform(1.0, 1.5))
            
            # 3-2. ë‹¤ì‹œ ë°”ë‹¥ìœ¼ë¡œ ë‚´ë¦¼
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(random.uniform(2.0, 3.0))
            
        return True
    except Exception as e:
        print(f"   [Scroll Error] {e}")
        return False

def load_cookies(driver, filename):
    if not os.path.exists(filename): return False
    try:
        with open(filename, "rb") as file:
            cookies = pickle.load(file)
            for cookie in cookies: driver.add_cookie(cookie)
        return True
    except: return False

def save_cookies(driver, filename):
    with open(filename, "wb") as file: pickle.dump(driver.get_cookies(), file)

def wait_for_login(driver):
    print("\n[USER ACTION] ë¡œê·¸ì¸ í•„ìš”! (3ë¶„ ëŒ€ê¸°)")
    start = time.time()
    while time.time() - start < 180:
        if "home" in driver.current_url or "explore" in driver.current_url:
            save_cookies(driver, COOKIE_FILE)
            print("[SUCCESS] ë¡œê·¸ì¸ ê°ì§€ ì™„ë£Œ.")
            return True
        time.sleep(1)
    return False

# ==========================================
# 2. í‚¤ì›Œë“œ í™•ì¥ ë° ì¿¼ë¦¬ ìƒì„± (ìˆ˜ì •ë¨)
# ==========================================
def clean_keyword(text):
    """íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì •ì œ"""
    return re.sub(r'[^\w\s]', '', str(text)).strip()

def build_smart_query(keyword):
    """
    í‚¤ì›Œë“œ ê¸¸ì´ì— ë”°ë¼ ê²€ìƒ‰ ì „ëµì„ ë‹¤ë¥´ê²Œ ê°€ì ¸ê°
    """
    keyword = clean_keyword(keyword)
    tokens = keyword.split()
    
    # Case 1: 1ë‹¨ì–´ì§œë¦¬ (ì˜ˆ: "Python", "ì„œë²„")
    # -> ê¸°ì¡´ì²˜ëŸ¼ í™•ì¥í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ 
    if len(tokens) == 1:
        return f'"{keyword}"'  # 1ë‹¨ì–´ëŠ” ì •í™•ë„ë¥¼ ìœ„í•´ ë”°ì˜´í‘œ ì¶”ì²œ
        
    # Case 2: 2ë‹¨ì–´ ì´ìƒ (ì˜ˆ: "ìƒì„±í˜• AI ëª¨ë¸", "ë¦¬ì•¡íŠ¸ ìƒíƒœ ê´€ë¦¬")
    # -> ë”°ì˜´í‘œë¥¼ ì“°ë©´ ê²°ê³¼ê°€ 0ì´ ë‚˜ì˜¤ë¯€ë¡œ, ë”°ì˜´í‘œ ì—†ì´ AND ì¡°ê±´ìœ¼ë¡œ ë¬¶ìŒ
    # -> íŠ¸ìœ„í„°ì—ì„œ (A B) ë¼ê³  ì“°ë©´ (A AND B)ë¡œ ë™ì‘í•¨
    else:
        # ì „ëµ: "ì •í™•í•œ êµ¬ë¬¸" OR (ë‹¨ì–´ AND ë‹¨ì–´)
        # ì˜ˆ: "ìƒì„±í˜• AI" OR (ìƒì„±í˜• AI)
        loose_match = f"{' '.join(tokens)}" # ë”°ì˜´í‘œ ì—†ëŠ” ë²„ì „
        
        # ë‘ ê°€ì§€ ê²½ìš°ë¥¼ ë‹¤ ì°¾ë˜, loose_matchê°€ ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´
        return f"{loose_match}"

def generate_queries(csv_path, max_query_length=500):
    print("\n" + "="*60)
    print("ğŸ“‚ [Step 1] ì§€ëŠ¥í˜• ì¿¼ë¦¬ ìƒì„± (Long-tail í‚¤ì›Œë“œ êµ¬ì¶œ ì‘ì „)")
    print("="*60)
    
    if not os.path.exists(csv_path): return []

    try:
        df = pd.read_csv(csv_path)
        # ì»¬ëŸ¼ëª… ì°¾ê¸° (keywordê°€ í¬í•¨ëœ ì»¬ëŸ¼)
        col = [c for c in df.columns if 'keyword' in c.lower()][0]
        raw_keywords = df[col].dropna().unique().tolist()
        random.shuffle(raw_keywords)
        
        query_groups = []
        
        for keyword in raw_keywords:
            # [í•µì‹¬] ìŠ¤ë§ˆíŠ¸ ì¿¼ë¦¬ ë¹Œë” ì‚¬ìš©
            core_query = build_smart_query(keyword)
            
            # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
            from datetime import datetime, timedelta
            end_date = datetime.now()
            start_date = end_date - timedelta(days=SEARCH_DAYS)
            
            # ì¿¼ë¦¬ ì¡°ë¦½
            parts = [
                core_query,  # ("ìƒì„±í˜• AI" OR (ìƒì„±í˜• AI)) í˜•íƒœ
                # LANG_FILTER, 
                # "-filter:retweets", 
                # f"min_retweets:{MIN_RETWEETS}",
                # ì œì™¸ì–´ëŠ” ê°€ì¥ í•µì‹¬ì ì¸ ê²ƒ 5ê°œë§Œ (ì¿¼ë¦¬ ê¸¸ì´ ì ˆì•½)
                # "-ì–‘ë„ -ë‚˜ëˆ” -í¬ì¹´ -í† í†  -ëŒ€ì¶œ", 
                # f"since:{start_date.strftime('%Y-%m-%d')}",
                # f"until:{end_date.strftime('%Y-%m-%d')}"
            ]
        
            full_query_string = " ".join(parts)
            
            # ê¸¸ì´ ì²´í¬
            if len(full_query_string) > max_query_length:
                print(f"   [Skip] ì¿¼ë¦¬ ë„ˆë¬´ ê¸¸ìŒ: {keyword[:30]}...")
                continue
                
            query_groups.append((full_query_string, [keyword]))
            
        print(f"   -> ì´ {len(query_groups)}ê°œ ìŠ¤ë§ˆíŠ¸ ì¿¼ë¦¬ ìƒì„±")
        return query_groups
        
    except Exception as e:
        print(f"Error: {e}")
        return []
# ==========================================
# 3. ë°ì´í„° íŒŒì‹±
# ==========================================
def parse_number(text):
    if not text: return 0
    text = text.replace(',', '')
    try:
        if 'K' in text: return int(float(text.replace('K', '')) * 1000)
        if 'M' in text: return int(float(text.replace('M', '')) * 1000000)
        nums = re.findall(r'\d+', text)
        return int(nums[0]) if nums else 0
    except: return 0

def parse_tweet(driver, article):
    try:
        text_elem = article.find_element(By.CSS_SELECTOR, 'div[data-testid="tweetText"]')
        text = text_elem.text
        if not text: return None
        
        try: dt = article.find_element(By.TAG_NAME, "time").get_attribute("datetime")
        except: dt = datetime.now().isoformat()
        
        metrics = {'reply': 0, 'retweet': 0, 'like': 0, 'view': 0}
        
        for m_key, testid in [('reply', 'reply'), ('retweet', 'retweet'), ('like', 'like')]:
            try:
                btn = article.find_element(By.CSS_SELECTOR, f'button[data-testid="{testid}"]')
                metrics[m_key] = parse_number(btn.get_attribute("aria-label"))
            except: 
                if m_key == 'like':
                    try:
                        btn = article.find_element(By.CSS_SELECTOR, 'button[data-testid="unlike"]')
                        metrics['like'] = parse_number(btn.get_attribute("aria-label"))
                    except: pass

        try:
            link = article.find_element(By.CSS_SELECTOR, 'a[href*="/analytics"]')
            metrics['view'] = parse_number(link.get_attribute("aria-label"))
        except: pass

        # [ìˆ˜ì •] íŒ”ë¡œì›Œ ìˆ˜ ëŒ€ì‹  ì‘ì„±ì ID(í•¸ë“¤) ìˆ˜ì§‘
        author_id = "unknown"
        try:
            user_link = article.find_element(By.CSS_SELECTOR, 'div[data-testid="User-Name"] a')
            href = user_link.get_attribute("href")
            if href:
                # https://twitter.com/username -> @username
                author_id = "@" + href.split('/')[-1]
        except: pass

        return {
            'text': text,
            'created_at': dt,
            'reply': metrics['reply'],
            'retweet': metrics['retweet'],
            'like': metrics['like'],
            'view': metrics['view'],
            'author_id': author_id  # [ë³€ê²½] follower_count -> author_id
        }
    except: return None

# ==========================================
# 4. í•„í„°ë§ ë° ìˆ˜ì§‘
# ==========================================
def is_clean_content(text):
    text = str(text).lower()
    for noise in ALL_NOISE_KEYWORDS:
        if noise in text: return False
    return True

def detect_keyword_in_text(text, keyword_list):
    text_lower = text.lower()
    for k in keyword_list:
        if k.lower() in text_lower:
            return k
    return keyword_list[0] 

def perform_search_and_collect(driver, query_string, group_keywords, limit):
    try:
        # [ìˆ˜ì •] í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì¿¼ë¦¬ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        print(f"\n   [ğŸ” Current Query] {query_string[:100]}... (Total len: {len(query_string)})")
        
        encoded = urllib.parse.quote(query_string)
        driver.get(f"https://twitter.com/search?q={encoded}&src=typed_query&f={SEARCH_MODE}")
        
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'article[data-testid="tweet"]'))
            )
        except: 
            # [ì¶”ê°€] ë¦¬ë¯¸íŠ¸ ê°ì§€
            page_source = driver.page_source
            if "ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤" in page_source or "Rate limit exceeded" in page_source:
                print("\n" + "="*60)
                print("ğŸš¨ [CRITICAL] íŠ¸ìœ„í„° ìš”ì²­ í•œë„ ì´ˆê³¼ (Rate Limit Exceeded)")
                print("   -> 15ë¶„ê°„ ëŒ€ê¸° í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... (ì»¤í”¼ í•œ ì” í•˜ê³  ì˜¤ì„¸ìš” â˜•ï¸)")
                print("="*60)
                time.sleep(900) # 15ë¶„ ëŒ€ê¸°
                return [] # ì´ë²ˆ ì¿¼ë¦¬ëŠ” ê±´ë„ˆë›°ê±°ë‚˜, ì¬ì‹œë„ ë¡œì§ì„ ìƒìœ„ì— êµ¬í˜„í•´ì•¼ í•¨ (ì¼ë‹¨ì€ ìŠ¤í‚µ)
            
            print("   -> âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ.")
            # print(f"   -> [Debug] Current URL: {driver.current_url}")
            # driver.save_screenshot("debug_search_fail.png")
            return []
            
        collected = []
        seen_texts = set()
        last_height = driver.execute_script("return document.body.scrollHeight")
        stuck = 0
        consecutive_retries = 0  # [ì¶”ê°€] ì—°ì† ì¬ì‹œë„ íšŸìˆ˜ ì œí•œ
        
        while len(collected) < limit:
            articles = driver.find_elements(By.CSS_SELECTOR, 'article[data-testid="tweet"]')
            found_new = False
            
            # [Debug] í•„í„°ë§ í†µê³„
            scanned_count = 0
            filtered_noise = 0
            filtered_seen = 0
            
            for art in articles:
                if len(collected) >= limit: break
                
                # [ìˆ˜ì •] driver ì „ë‹¬
                data = parse_tweet(driver, art)
                scanned_count += 1
                
                if not data: continue
                
                # 1. ë…¸ì´ì¦ˆ í•„í„°ë§ ì²´í¬
                if not is_clean_content(data['text']):
                    filtered_noise += 1
                    # print(f"      [Noise Filtered] {data['text'][:30]}...") # ë„ˆë¬´ ì‹œë„ëŸ¬ìš°ë©´ ì£¼ì„ ì²˜ë¦¬
                    continue
                    
                # 2. ì¤‘ë³µ ì²´í¬
                sig = data['text'][:50]
                if sig in seen_texts:
                    filtered_seen += 1
                    continue
                    
                seen_texts.add(sig)
                data['search_keyword'] = detect_keyword_in_text(data['text'], group_keywords)
                data['search_query'] = query_string 
                collected.append(data)
                found_new = True
            
            # [Debug] ì´ë²ˆ ìŠ¤í¬ë¡¤ ê²°ê³¼ ì¶œë ¥
            if scanned_count > 0:
                print(f"      -> ìŠ¤ìº”: {scanned_count}ê°œ | ìˆ˜ì§‘: {found_new} | ë…¸ì´ì¦ˆ: {filtered_noise} | ì¤‘ë³µ: {filtered_seen}")
            
            if len(collected) >= limit: break
            
            # ... (ë°ì´í„° ìˆ˜ì§‘ ì½”ë“œ ë°”ë¡œ ë’¤) ...

            # 1. ìŠ¤ë§ˆíŠ¸ ìŠ¤í¬ë¡¤ ì‹¤í–‰ (stuck ìƒíƒœ ì „ë‹¬)
            smart_scroll(driver, last_height, stuck)
            
            # 2. ë†’ì´ ì²´í¬
            new_height = driver.execute_script("return document.body.scrollHeight")
            
            # 3. ë©ˆì¶¤ íŒë³„ ë¡œì§ ê°•í™”
            if new_height == last_height:
                stuck += 1
                print(f"      [Stuck {stuck}/7] ë¡œë”© ëŒ€ê¸° ì¤‘...")
                
                # [ìˆ˜ì •] ë§‰íˆë©´ ì¦‰ì‹œ 'ë‹¤ì‹œ ì‹œë„' ë²„íŠ¼ ì°¾ê¸° ì‹œë„ (ë‹¨, ì—°ì† 3íšŒê¹Œì§€ë§Œ)
                # [ìˆ˜ì •] ë§‰íˆë©´ ì¦‰ì‹œ 'ë‹¤ì‹œ ì‹œë„' ë²„íŠ¼ ì°¾ê¸° ì‹œë„
                if consecutive_retries < 3:
                    try:
                        retry_selectors = [
                            "//span[text()='ë‹¤ì‹œ ì‹œë„']",
                            "//span[contains(text(), 'Retry')]",
                            "//div[@role='button']//span[contains(text(), 'ë‹¤ì‹œ ì‹œë„')]",
                            "//span[contains(text(), 'ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”')]" # [ì¶”ê°€] ì •í™•í•œ ë¬¸êµ¬
                        ]
                        
                        clicked_retry = False
                        for sel in retry_selectors:
                            try:
                                retry_btn = driver.find_element(By.XPATH, sel)
                                driver.execute_script("arguments[0].click();", retry_btn)
                                print(f"      -> ğŸ”„ 'ë‹¤ì‹œ ì‹œë„' ë²„íŠ¼ í´ë¦­ ì„±ê³µ ({sel})")
                                stuck = 0 # ì„±ê³µ ì‹œ ë¦¬ì…‹
                                consecutive_retries += 1 # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
                                clicked_retry = True
                                time.sleep(random.uniform(3.0, 5.0)) # [ìˆ˜ì •] ëŒ€ê¸° ì‹œê°„ ì•½ê°„ ì¦ê°€
                                break
                            except: continue
                        
                        if clicked_retry:
                            continue # ì¬ì‹œë„ í–ˆìœ¼ë©´ ìŠ¤í¬ë¡¤ ì²´í¬ ê±´ë„ˆë›°ê³  ë‹¤ì‹œ ìŠ¤ìº”
                    except: pass
                
                # [ì¶”ê°€] 3íšŒ ì´ìƒ ì—°ì† ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ -> í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ (Soft Refresh)
                elif consecutive_retries >= 3:
                    print("      -> âš ï¸ ì—°ì† ì¬ì‹œë„ ì‹¤íŒ¨. í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ ì‹œë„...")
                    driver.refresh()
                    time.sleep(random.uniform(5.0, 8.0))
                    stuck = 0
                    consecutive_retries = 0
                    continue
                
                else:
                    print("      -> âš ï¸ ì¬ì‹œë„ í•œë„ ì´ˆê³¼ (ë¬´í•œ ë£¨í”„ ë°©ì§€)")
                
                # 7ë²ˆ ì´ìƒ ë§‰íˆë©´ í¬ê¸° (ë‹¤ìŒ í‚¤ì›Œë“œë¡œ)
                if stuck > 7: 
                    print("   -> âš ï¸ ìŠ¤í¬ë¡¤ ë ë„ë‹¬ (ë” ì´ìƒ ë°ì´í„° ì—†ìŒ)")
                    break
            else:
                stuck = 0
                consecutive_retries = 0 # ë†’ì´ê°€ ë³€í–ˆìœ¼ë©´ ì¬ì‹œë„ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
                last_height = new_height
                
            # ... (ì§„í–‰ë¥  í‘œì‹œ ì½”ë“œ) ...
                
            if len(collected) % 50 == 0 and found_new:
                print(f"      [{len(collected)}/{limit}] ìˆ˜ì§‘ ì¤‘...")
                
        return collected
    except Exception as e: 
        print(f"   [Error] {e}")
        return []

# ==========================================
# 5. ë©”ì¸
# ==========================================
def main():
    query_groups = generate_queries(CSV_FILE_PATH)
    if not query_groups: return
    
    options = uc.ChromeOptions()
    options.add_argument('--no-first-run')
    options.add_argument('--blink-settings=imagesEnabled=false')
    
    driver = uc.Chrome(options=options)
    
    try:
        driver.get("https://twitter.com")
        load_cookies(driver, COOKIE_FILE)
        driver.refresh()
        random_sleep(3, 5)
        
        if "login" in driver.current_url:
            if not wait_for_login(driver): return
            
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        output_dir = "data/twitter"
        os.makedirs(output_dir, exist_ok=True)
        filename = os.path.join(output_dir, f"twitter_retweet_filtered_{timestamp}.csv")
        total = 0
        
        print("="*60)
        print(f"ğŸš€ íŠ¸ìœ„í„° ìˆ˜ì§‘ ì‹œì‘ (Min Retweets: {MIN_RETWEETS}, 3ê¸€ì ë¯¸ë§Œ ë¶„í•  ê¸ˆì§€)")
        print("="*60)
        
        # [ìˆ˜ì •] follower_count -> author_id
        columns = ['text', 'reply', 'retweet', 'like', 'view', 'author_id', 'created_at', 'search_keyword', 'search_query']
        
        for idx, (q_str, k_list) in enumerate(query_groups, 1):
            print(f"\n[Group {idx}/{len(query_groups)}] ì‹œì‘")
            
            data = perform_search_and_collect(driver, q_str, k_list, TWEETS_PER_QUERY_GROUP)
            
            if data:
                df = pd.DataFrame(data)
                for col in columns:
                    if col not in df.columns:
                        df[col] = 0 if col in ['reply','retweet','like','view'] else ("" if col == 'author_id' else "")
                df = df[columns]
                
                header = not os.path.exists(filename)
                df.to_csv(filename, index=False, mode='a', encoding='utf-8-sig', header=header)
                total += len(data)
                print(f"   -> âœ… {len(data)}ê°œ ì €ì¥ (ëˆ„ì : {total})")
            
            if idx < len(query_groups):
                random_sleep(10, 15)
                
        print(f"\nğŸ‰ ì™„ë£Œ! ì´ {total}ê°œ ì €ì¥ë¨.\nğŸ“ {filename}")
        
    finally:
        driver.quit()

if __name__ == "__main__":
    main()