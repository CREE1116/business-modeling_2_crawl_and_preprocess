import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time
import re
import requests
import xml.etree.ElementTree as ET
from datetime import datetime
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np
import warnings

# ê²½ê³  ë¬´ì‹œ (LDA ì‹¤í–‰ ì‹œ ë°œìƒí•˜ëŠ” DeprecationWarning ë“±)
warnings.filterwarnings("ignore")

# ==========================================
# [ì„¤ì •] LDA ë¶„ì„ ë° í’ˆì§ˆ ê´€ë¦¬ íŒŒë¼ë¯¸í„°
# ==========================================
NUM_TOPICS = 10              # ì¶”ì¶œí•  í† í”½(ì£¼ì œ) ê°œìˆ˜
WORDS_PER_TOPIC = 5         # ê° í† í”½ë‹¹ ì¶”ì¶œí•  ëŒ€í‘œ í‚¤ì›Œë“œ ìˆ˜
MIN_KEYWORD_LENGTH = 2      # 2ê¸€ì ë¯¸ë§Œ ë‹¨ì–´ ì œì™¸ (ì˜ë¯¸ ì—†ëŠ” ì¡°ì‚¬ ë“± í•„í„°ë§)

# [ì¤‘ìš”] ë¶ˆìš©ì–´(Stopwords) ë¦¬ìŠ¤íŠ¸
# ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì—ì„œ í”íˆ ë‚˜ì˜¤ì§€ë§Œ ë¶„ì„ ê°€ì¹˜ëŠ” ì—†ëŠ” ë‹¨ì–´ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.
STOPWORDS = [
    'ë‰´ìŠ¤', 'ì†ë³´', 'ë‹¨ë…', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ë°œí‘œ', 'ê³µê°œ', 
    'ì˜ìƒ', 'ë…¼ë€', 'ì´ìœ ', 'ì¶©ê²©', 'ê²°êµ­', 'ì§„ì§œ', 'ê·¼í™©', 'ì˜ˆì •',
    'ê´€ë ¨', 'íŠ¹ì§•', 'ê°€ì¥', 'ëŒ€í•´', 'ìœ„í•´', 'í†µí•´', 'ë•Œë¬¸', 'ê²½ìš°',
    'ì •ë„', 'ìµœê·¼', 'ì§€ê¸ˆ', 'ë¬´ì—‡', 'ì–´ë–»ê²Œ', 'ë‹¤ì‹œ', 'ê³„ì†', 'ì¢…í•©',
    'ì¶œì‹œ', 'ê³µì‹', 'ì „ë§', 'ë¶„ì„', 'ì‹œì¥', 'ì„¸ê³„', 'êµ­ë‚´', 'í•œêµ­',
    'ì£¼ìš”', 'ìµœê³ ', 'ëŒ€ë¹„', 'ì‹œì‘', 'ê°œìµœ', 'ì§„í–‰', 'ì°¸ì—¬', 'ë“±ì¥'
]

# Kiwi (Python ì „ìš© í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°) ë¡œë“œ
# Java ì˜ì¡´ì„± ì—†ì´ ë¹ ë¥´ê³  ì •í™•í•¨
USE_KIWI = False
try:
    from kiwipiepy import Kiwi
    kiwi = Kiwi()
    USE_KIWI = True
    print("[System] Kiwi ë¡œë“œ ì„±ê³µ. ê³ í’ˆì§ˆ ëª…ì‚¬ ì¶”ì¶œ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
except ImportError:
    print("[System] kiwipiepyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("         -> ë‹¨ìˆœ ì •ê·œí‘œí˜„ì‹ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
    print("         -> pip install kiwipiepy ê¶Œì¥.")
except Exception as e:
    print(f"[System] Kiwi ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    print("         -> ë‹¨ìˆœ ì •ê·œí‘œí˜„ì‹ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")

# ==========================================
# 1. 4ëŒ€ ì†ŒìŠ¤ ì›ì²œ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
# ==========================================
def collect_raw_trend_data():
    print("\n" + "="*60)
    print("ğŸ“¡ [Phase 1] 4ëŒ€ ì†ŒìŠ¤ ì›ì²œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print("="*60)
    
    raw_data = []
    
    # Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™” (Google Trendsë„ Seleniumìœ¼ë¡œ ìˆ˜ì§‘í•˜ê¸° ìœ„í•¨)
    options = uc.ChromeOptions()
    options.add_argument('--headless') # í™”ë©´ ì—†ì´ ì‹¤í–‰
    options.add_argument('--no-first-run')
    
    driver = None
    try:
        driver = uc.Chrome(options=options)
        
        # [1] Google Trends (Daily - HTML Scraping)
        # RSSê°€ 404/ì°¨ë‹¨ë˜ëŠ” ê²½ìš°ê°€ ë§ì•„ Seleniumìœ¼ë¡œ HTML í˜ì´ì§€ ì§ì ‘ í¬ë¡¤ë§
        print("   -> [1/4] Google Trends (Daily) ìˆ˜ì§‘ ì¤‘... (Selenium)")
        try:
            url = "https://trends.google.co.kr/trends/trendingsearches/daily?geo=KR&hl=ko"
            driver.get(url)
            time.sleep(5) # ë¡œë”© ëŒ€ê¸°
            
            # [ìˆ˜ì •] google_trend.pyì™€ ë™ì¼í•œ selector ì‚¬ìš©
            # feed-itemì´ ì•„ë‹ˆë¼ tr[role='row'] êµ¬ì¡°ì„
            rows = driver.find_elements(By.CSS_SELECTOR, "tr[role='row']")
            
            for row in rows:
                try:
                    # í‚¤ì›Œë“œ ì¶”ì¶œ (mZ3RIc í´ë˜ìŠ¤ ë˜ëŠ” í…Œì´ë¸” êµ¬ì¡°)
                    try:
                        keyword_elem = row.find_element(By.CLASS_NAME, "mZ3RIc")
                        keyword = keyword_elem.text.strip()
                    except:
                        # Fallback
                        keyword_elem = row.find_element(By.CSS_SELECTOR, "td:nth-child(2) > div")
                        keyword = keyword_elem.text.strip()
                        
                    if keyword:
                        raw_data.append(keyword)
                except:
                    continue
                    
            print(f"      - {len(rows)}ê°œ í‚¤ì›Œë“œ í™•ë³´")
            
        except Exception as e:
            print(f"      [Error] Google Trends ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        # [2] Google News RSS (Technology, Science, Business)
        # ë°ì´í„° ì†ŒìŠ¤ í™•ì¥ (ì‚¬ìš©ì ìš”ì²­)
        print("   -> [2/4] Google News (Tech/Biz/Sci) ìˆ˜ì§‘ ì¤‘...")
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
        }
        
        # ì—¬ëŸ¬ ì„¹ì…˜ ìˆ˜ì§‘
        sections = {
            'TECHNOLOGY': 'https://news.google.com/rss/headlines/section/topic/TECHNOLOGY?hl=ko&gl=KR&ceid=KR:ko',
            'BUSINESS': 'https://news.google.com/rss/headlines/section/topic/BUSINESS?hl=ko&gl=KR&ceid=KR:ko',
            'SCIENCE': 'https://news.google.com/rss/headlines/section/topic/SCIENCE?hl=ko&gl=KR&ceid=KR:ko'
        }
        
        for sec_name, url in sections.items():
            try:
                response = requests.get(url, headers=headers, timeout=10)
                root = ET.fromstring(response.content)
                count = 0
                for item in root.findall(".//item"):
                    title = item.find("title").text.split(" - ")[0]
                    raw_data.append(title)
                    count += 1
                print(f"      - {sec_name}: {count}ê°œ")
            except Exception as e:
                print(f"      [Error] {sec_name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        # [3] & [4] Naver News (IT/Science & Ranking)
        print("   -> [3/4, 4/4] Naver News (IT/Science & Ranking) ìˆ˜ì§‘ ì¤‘...")
        try:
            # Naver IT ì¼ë°˜ ë‰´ìŠ¤
            driver.get("https://news.naver.com/section/105")
            time.sleep(3)
            headlines = driver.find_elements(By.CSS_SELECTOR, "a strong, .sa_text_strong, .sh_text_headline")
            for h in headlines:
                text = h.text.strip()
                if len(text) > 4: raw_data.append(text)
                
            # Naver Ranking (IT/Science)
            driver.get("https://news.naver.com/main/ranking/popularDay.naver")
            time.sleep(2)
            rankings = driver.find_elements(By.CSS_SELECTOR, ".list_content a")
            for r in rankings:
                text = r.text.strip()
                if len(text) > 4: raw_data.append(text)
                
        except Exception as e:
            print(f"      [Error] Naver ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            
    except Exception as e:
        print(f"   [Critical Error] ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    finally:
        if driver: driver.quit()
        
    # ì¤‘ë³µ ì œê±°
    raw_data = list(set(raw_data))
    print(f"   -> [ì™„ë£Œ] ì´ {len(raw_data)}ê°œì˜ ì›ì²œ í—¤ë“œë¼ì¸ í™•ë³´.")
    return raw_data

# ==========================================
# 2. í† í¬ë‚˜ì´ì € (ëª…ì‚¬ ì¶”ì¶œ + ë¶ˆìš©ì–´ ì²˜ë¦¬)
# ==========================================
def tokenizer(text):
    if USE_KIWI:
        # í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ëª…ì‚¬(NNG, NNP, NR, NP)ë§Œ ì¶”ì¶œ
        # KiwiëŠ” (token, tag, start, len) íŠœí”Œì„ ë°˜í™˜í•˜ê±°ë‚˜ Token ê°ì²´ ë°˜í™˜
        tokens = kiwi.tokenize(text)
        nouns = [t.form for t in tokens if t.tag.startswith('N')] # ëª…ì‚¬ ê³„ì—´ íƒœê·¸ (NNG, NNP ë“±)
        
        # 2ê¸€ì ì´ìƒì´ê³  ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ ê²ƒë§Œ í•„í„°ë§
        return [n for n in nouns if len(n) >= MIN_KEYWORD_LENGTH and n not in STOPWORDS]
    else:
        # Fallback: ì •ê·œí‘œí˜„ì‹ (í•œê¸€/ì˜ì–´/ìˆ«ì 2ê¸€ì ì´ìƒ)
        tokens = re.findall(r'[ê°€-í£a-zA-Z0-9]{2,}', text)
        return [t for t in tokens if t not in STOPWORDS]

# ==========================================
# 3. LDA ë¶„ì„ ë° í’ˆì§ˆ í‰ê°€ (Grid Search + Coherence)
# ==========================================
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
import gensim
from gensim.corpora import Dictionary
from gensim.models import CoherenceModel

def calculate_coherence_score(sklearn_lda_model, dtm, vectorizer, tokenized_docs):
    """
    Sklearn LDA ëª¨ë¸ì˜ Coherence Score(c_v)ë¥¼ Gensimì„ ì´ìš©í•´ ê³„ì‚°
    """
    try:
        # 1. í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ
        feature_names = vectorizer.get_feature_names_out()
        topics = []
        for topic_idx, topic in enumerate(sklearn_lda_model.components_):
            top_indices = topic.argsort()[:-WORDS_PER_TOPIC - 1:-1]
            top_words = [feature_names[i] for i in top_indices]
            topics.append(top_words)
            
        # 2. Gensim Dictionary ìƒì„±
        dictionary = Dictionary(tokenized_docs)
        
        # 3. Coherence Model ìƒì„± (c_v)
        cm = CoherenceModel(topics=topics, texts=tokenized_docs, dictionary=dictionary, coherence='c_v')
        return cm.get_coherence()
    except Exception as e:
        print(f"   [Warning] Coherence ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0

def extract_keywords_with_lda(docs):
    print("\n" + "="*60)
    print("ğŸ§  [Phase 2] TF-IDF í•„í„°ë§ & LDA ë¶„ì„ (Grid Search + Coherence)")
    print("="*60)
    
    if not docs: return []

    # 0. ì „ì²˜ë¦¬ (í† í°í™”)
    tokenized_docs = [tokenizer(doc) for doc in docs]
    # ë¹ˆ ë¬¸ì„œ ì œê±°
    tokenized_docs = [doc for doc in tokenized_docs if doc]
    # ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ê²°í•© (Sklearn ì…ë ¥ìš©)
    preprocessed_docs = [" ".join(doc) for doc in tokenized_docs]

    # 1. TF-IDF ë²¡í„°í™” (ì¤‘ìš”ë„ ë‚®ì€ ë‹¨ì–´ í•„í„°ë§ íš¨ê³¼)
    try:
        # min_df=2: ìµœì†Œ 2ë²ˆ ì´ìƒ ë“±ì¥
        # max_df=0.8: 80% ì´ìƒ ë¬¸ì„œì— ë“±ì¥í•˜ë©´ ì œì™¸ (ë¶ˆìš©ì–´ ì„±ê²©)
        vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), preprocessor=lambda x: x, max_df=0.8, min_df=2)
        dtm = vectorizer.fit_transform(preprocessed_docs)
        
        vocab_size = len(vectorizer.get_feature_names_out())
        print(f"   -> ì¶”ì¶œëœ ê³ ìœ  ëª…ì‚¬ ê°œìˆ˜: {vocab_size}ê°œ")
        
        if vocab_size < 10:
            print("   [Error] ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ ë¶„ì„ ë¶ˆê°€.")
            return docs[:5]

        # 2. Grid Searchë¡œ ìµœì ì˜ í† í”½ ìˆ˜ ì°¾ê¸°
        print("   -> ìµœì ì˜ í† í”½ ìˆ˜ íƒìƒ‰ ì¤‘ (Grid Search)...")
        
        search_params = {'n_components': [3, 4, 5, 6, 7, 8]}
        
        lda = LatentDirichletAllocation(random_state=42, learning_method='online', learning_offset=50.)
        
        model = GridSearchCV(lda, param_grid=search_params, cv=3, verbose=1)
        model.fit(dtm)
        
        best_lda_model = model.best_estimator_
        best_n_topics = model.best_params_['n_components']
        
        # 3. Coherence Score ê³„ì‚°
        coherence_score = calculate_coherence_score(best_lda_model, dtm, vectorizer, tokenized_docs)
        
        print(f"\n   ğŸ† [Best Model Found]")
        print(f"      - Best Topic Count: {best_n_topics}")
        print(f"      - Best Log Likelihood: {model.best_score_:.2f}")
        print(f"      - Perplexity: {best_lda_model.perplexity(dtm):.2f}")
        print(f"      - Coherence Score (c_v): {coherence_score:.4f} (0.5 ì´ìƒì´ë©´ ì¢‹ìŒ)")

        # 4. í‚¤ì›Œë“œ ì¶”ì¶œ
        feature_names = vectorizer.get_feature_names_out()
        extracted_keywords = set()
        
        print(f"\n   ğŸ” [Topic Keywords Extraction (Topics={best_n_topics})]")
        for idx, topic in enumerate(best_lda_model.components_):
            top_indices = topic.argsort()[:-WORDS_PER_TOPIC - 1:-1]
            top_words = [feature_names[i] for i in top_indices]
            
            print(f"      Topic {idx+1}: {top_words}")
            extracted_keywords.update(top_words)
            
        return list(extracted_keywords)

    except ValueError as e:
        print(f"   [Error] LDA ë¶„ì„ ì‹¤íŒ¨: {e}")
        return docs[:20] 

# ==========================================
# ë©”ì¸ ì‹¤í–‰
# ==========================================
if __name__ == "__main__":
    # 1. ìˆ˜ì§‘
    raw_headlines = collect_raw_trend_data()
    
    # 2. ë¶„ì„
    final_keywords = extract_keywords_with_lda(raw_headlines)
    
    # 3. ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    filename = f"trend_keywords_{timestamp}.csv"
    
    df = pd.DataFrame(final_keywords, columns=["keyword"])
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    
    print("\n" + "="*60)
    print(f"ğŸ‰ ê³ í’ˆì§ˆ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
    print(f"ğŸ“ íŒŒì¼ëª…: {filename}")
    print(f"ğŸ”‘ ìµœì¢… í‚¤ì›Œë“œ: {len(final_keywords)}ê°œ")
    print("="*60)