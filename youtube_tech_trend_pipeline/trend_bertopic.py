import requests
import xml.etree.ElementTree as ET
import pandas as pd
import os
from datetime import datetime
from kiwipiepy import Kiwi

# BERTopic ê´€ë ¨
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer

# ==========================================
# [ì„¤ì •] íŒŒë¼ë¯¸í„°
# ==========================================
MIN_CLUSTER_SIZE = 5        # HDBSCAN ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°
MIN_SAMPLES = 3             # HDBSCAN ìµœì†Œ ìƒ˜í”Œ ìˆ˜
KEYWORDS_PER_TOPIC = 10     # í† í”½ë‹¹ í‚¤ì›Œë“œ ê°œìˆ˜
MODEL_NAME = 'jhgan/ko-sroberta-multitask'  # í•œêµ­ì–´ SBERT

# ë¶ˆìš©ì–´
STOPWORDS = [
    # ì¼ë°˜ ë¶ˆìš©ì–´
    'ë‰´ìŠ¤', 'ì†ë³´', 'ë‹¨ë…', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ë°œí‘œ', 'ê³µê°œ',
    'ì˜ìƒ', 'ë…¼ë€', 'ì´ìœ ', 'ì¶©ê²©', 'ê²°êµ­', 'ì§„ì§œ', 'ê·¼í™©', 'ì˜ˆì •',
    'ê´€ë ¨', 'íŠ¹ì§•', 'ê°€ì¥', 'ëŒ€í•´', 'ìœ„í•´', 'í†µí•´', 'ë•Œë¬¸', 'ê²½ìš°',
    'ì •ë„', 'ìµœê·¼', 'ì§€ê¸ˆ', 'ë¬´ì—‡', 'ì–´ë–»ê²Œ', 'ë‹¤ì‹œ', 'ê³„ì†', 'ì¢…í•©',
    'ì •ë‹µ', 'í€´ì¦ˆ', 'ë¬¸ì œ', 'ì´ë²¤íŠ¸', 'ë‹¹ì²¨', 'ì°¸ê°€',
    'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°', 'ë­ì„', 'ê°œë…', 'ìƒê°', 'ì‚¬ëŒ',
    'ì§„ì§œ', 'ì •ë§', 'ë§¤ìš°', 'ë„ˆë¬´', 'ì™„ì „', 'ëŒ€ë°•', 'í—', 'ì™€',
    'ëŒ“ê¸€', 'ì¶”ì²œ', 'ë¹„ì¶”', 'ì‹ ê³ ', 'ì‚­ì œ', 'ì°¨ë‹¨',
    
    # íšŒì‚¬ëª…/ì„œë¹„ìŠ¤ëª… (ê¸°ìˆ  ë¸”ë¡œê·¸ ì¶œì²˜)
    'ë±…í¬', 'ë±…í¬ìƒëŸ¬ë“œ', 'ìƒëŸ¬ë“œ', 'ë±…í¬ ìƒëŸ¬ë“œ',
    'ì»¬ë¦¬', 'ë§ˆì¼“ì»¬ë¦¬', 'í—¬ë¡œìš° ì»¬ë¦¬',
    'ë°ë¸Œì‹œìŠ¤í„°ì¦ˆ',
    'ì¿ í‚¤ëŸ°', 'í‚¹ë¤', 'ì¿ í‚¤ëŸ° í‚¹ë¤',
    'ë¦¬ë””', 'ë¦¬ë””ë¶ìŠ¤',
    'í† ìŠ¤', 'ë¹„ë°”ë¦¬í¼ë¸”ë¦¬ì¹´',
    'ë‹¹ê·¼', 'ë‹¹ê·¼ë§ˆì¼“',
    'ì§ë°©',
    'ì™“ì± ', 'ì™“ì± í”Œë ˆì´',
    'ì˜ì¹´',
    'ìš”ê¸°ìš”',
    'ë¬´ì‹ ì‚¬',
    'í•˜ì´í¼ì»¤ë„¥íŠ¸',
    'ë„¤ì´ë²„', 'd2',
    'ë¼ì¸', 'line',
    'ì¿ íŒ¡',
    'nhn',
    'kurly',
    'toss',
]

# Kiwi ì´ˆê¸°í™”
try:
    kiwi = Kiwi()
    kiwi.add_user_word("ì œë¯¸ë‚˜ì´", "NNP")
    kiwi.add_user_word("ì±—GPT", "NNP")
    kiwi.add_user_word("ë°”ì´ë¸Œì½”ë”©", "NNP")
    print("[System] Kiwi ë¡œë“œ ì„±ê³µ")
except:
    kiwi = None
    print("[System] Kiwi ë¡œë“œ ì‹¤íŒ¨")

# ==========================================
# 1. ë°ì´í„° ìˆ˜ì§‘
# ==========================================
def collect_data():
    print("\n" + "="*60)
    print("ğŸ“¡ [Phase 1] ë°ì´í„° ìˆ˜ì§‘ (ê¸°ìˆ  ë¸”ë¡œê·¸ RSS)")
    print("="*60)
    
    raw_data = []
    headers = {"User-Agent": "Mozilla/5.0"}
    
    # ë‰´ìŠ¤ RSS
    news_rss = [
        ('ê¸±ë‰´ìŠ¤', 'https://feeds.feedburner.com/geeknews-feed'),
    ]
    
    # í•œêµ­ IT ê¸°ì—… ê¸°ìˆ  ë¸”ë¡œê·¸ RSS
    tech_blog_rss = [
        ('ë¬´ì‹ ì‚¬', 'https://medium.com/feed/musinsa-tech'),
        ('ë„¤ì´ë²„ D2', 'https://d2.naver.com/d2.atom'),
        ('ë§ˆì¼“ì»¬ë¦¬', 'https://helloworld.kurly.com/feed.xml'),
        ('ìš°ì•„í•œí˜•ì œë“¤', 'https://techblog.woowahan.com/feed'),
        ('ì¹´ì¹´ì˜¤ì—”í„°', 'https://tech.kakaoenterprise.com/feed'),
        ('ë°ë¸Œì‹œìŠ¤í„°ì¦ˆ', 'https://tech.devsisters.com/rss.xml'),
        ('ë¼ì¸', 'https://engineering.linecorp.com/ko/feed/index.html'),
        ('ì¿ íŒ¡', 'https://medium.com/feed/coupang-engineering'),
        ('ë‹¹ê·¼ë§ˆì¼“', 'https://medium.com/feed/daangn'),
        ('í† ìŠ¤', 'https://toss.tech/rss.xml'),
        ('ì§ë°©', 'https://medium.com/feed/zigbang'),
        ('ì™“ì± ', 'https://medium.com/feed/watcha'),
        ('ë±…í¬ìƒëŸ¬ë“œ', 'https://blog.banksalad.com/rss.xml'),
        ('Hyperconnect', 'https://hyperconnect.github.io/feed.xml'),
        ('ìš”ê¸°ìš”', 'https://techblog.yogiyo.co.kr/feed'),
        ('ì˜ì¹´', 'https://tech.socarcorp.kr/feed'),
        ('ë¦¬ë””', 'https://www.ridicorp.com/feed'),
        ('NHN Toast', 'https://meetup.toast.com/rss'),
        ('Velog', 'https://v2.velog.io/rss/'),
        ('ê°œë°œììŠ¤ëŸ½ë‹¤', 'https://blog.gaerae.com/feeds/posts/default?alt=rss'),
        ('44BITS', 'https://www.44bits.io/ko/feed/all'),
    ]
    
    all_rss = news_rss + tech_blog_rss
    
    print("   -> RSS ìˆ˜ì§‘ ì¤‘...")
    for name, url in all_rss:
        try:
            res = requests.get(url, headers=headers, timeout=10)
            root = ET.fromstring(res.content)
            count = 0
            
            # Atom í˜•ì‹
            if any(x in url for x in ['geeknews', 'd2.naver', 'hyperconnect']):
                for item in root.findall(".//{http://www.w3.org/2005/Atom}entry"):
                    try:
                        title_elem = item.find("{http://www.w3.org/2005/Atom}title")
                        if title_elem is not None and title_elem.text:
                            raw_data.append(title_elem.text)
                            count += 1
                    except: pass
            
            # RSS 2.0 í˜•ì‹
            if count == 0:
                for item in root.findall(".//item"):
                    try:
                        title_elem = item.find("title")
                        if title_elem is not None and title_elem.text:
                            raw_data.append(title_elem.text)
                            count += 1
                    except: pass
            
            if count > 0:
                print(f"      -> {name}: {count}ê°œ")
        except:
            pass
    
    unique_data = list(set(raw_data))
    print(f"   -> ì´ {len(unique_data)}ê°œ ë¬¸ì¥ í™•ë³´.\n")
    return unique_data

# ==========================================
# 2. BERTopic ë¶„ì„
# ==========================================
def extract_keywords_with_bertopic(documents):
    print("="*60)
    print("ğŸ§  [Phase 2] BERTopic ë¶„ì„")
    print("="*60)
    
    # 1. SBERT ëª¨ë¸ ë¡œë“œ
    print(f"   -> ëª¨ë¸ ë¡œë“œ: {MODEL_NAME}")
    embedding_model = SentenceTransformer(MODEL_NAME)
    
    # 2. UMAP ì°¨ì› ì¶•ì†Œ ì„¤ì •
    print("   -> UMAP ì°¨ì› ì¶•ì†Œ ì„¤ì •")
    umap_model = UMAP(
        n_neighbors=15,
        n_components=5,
        min_dist=0.0,
        metric='cosine',
        random_state=42
    )
    
    # 3. HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì •
    print("   -> HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì •")
    hdbscan_model = HDBSCAN(
        min_cluster_size=MIN_CLUSTER_SIZE,
        min_samples=MIN_SAMPLES,
        metric='euclidean',
        cluster_selection_method='eom',
        prediction_data=True
    )
    
    # 4. CountVectorizer ì„¤ì • (í•œêµ­ì–´)
    if kiwi:
        def korean_tokenizer(text):
            tokens = kiwi.tokenize(text)
            return [
                token.form for token in tokens
                if token.tag in ['NNG', 'NNP', 'SL', 'SH']  # ëª…ì‚¬, ì™¸ë˜ì–´, ì˜ì–´
                and len(token.form) >= 2
                and token.form not in STOPWORDS
            ]
    else:
        korean_tokenizer = None
    
    vectorizer_model = CountVectorizer(
        tokenizer=korean_tokenizer,
        stop_words=STOPWORDS if not kiwi else None,
        min_df=1,
        ngram_range=(1, 2)
    )
    
    # 5. BERTopic ëª¨ë¸ ìƒì„±
    print("   -> BERTopic ëª¨ë¸ ìƒì„± ë° í•™ìŠµ ì¤‘...\n")
    topic_model = BERTopic(
        embedding_model=embedding_model,
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        vectorizer_model=vectorizer_model,
        top_n_words=KEYWORDS_PER_TOPIC,
        language='korean',
        calculate_probabilities=False,
        verbose=True
    )
    
    # 6. í† í”½ ì¶”ì¶œ
    topics, probs = topic_model.fit_transform(documents)
    
    # 7. ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ” [BERTopic Results]")
    print("="*60)
    
    topic_info = topic_model.get_topic_info()
    print(f"\nâœ… ë°œê²¬ëœ í† í”½ ê°œìˆ˜: {len(topic_info) - 1}ê°œ (-1 ì œì™¸)")
    print(f"âœ… ë…¸ì´ì¦ˆ ë¬¸ì„œ: {topic_info[topic_info['Topic'] == -1]['Count'].values[0]}ê°œ\n")
    
    # ê° í† í”½ ì¶œë ¥
    all_keywords = []
    for idx, row in topic_info.iterrows():
        topic_id = row['Topic']
        if topic_id == -1:  # ë…¸ì´ì¦ˆ ì œì™¸
            continue
        
        count = row['Count']
        keywords = topic_model.get_topic(topic_id)
        
        if keywords:
            print(f"ğŸ“‚ Topic {topic_id} ({count}ê°œ ë¬¸ì„œ)")
            keyword_list = [word for word, score in keywords[:KEYWORDS_PER_TOPIC]]
            print(f"   í‚¤ì›Œë“œ: {keyword_list}\n")
            all_keywords.extend(keyword_list)
    
    return all_keywords, topic_model

# ==========================================
# 3. ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    # 1. ë°ì´í„° ìˆ˜ì§‘
    documents = collect_data()
    
    if len(documents) < 10:
        print("âŒ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. (ìµœì†Œ 10ê°œ í•„ìš”)")
        return
    
    # 2. BERTopic ë¶„ì„
    keywords, topic_model = extract_keywords_with_bertopic(documents)
    
    # 3. êµ¬ì¡°í™”ëœ ê²°ê³¼ ìƒì„±
    topic_info = topic_model.get_topic_info()
    
    # í† í”½ë³„ë¡œ í‚¤ì›Œë“œ ì •ë¦¬
    topic_keywords = []
    for idx, row in topic_info.iterrows():
        topic_id = row['Topic']
        if topic_id == -1:  # ë…¸ì´ì¦ˆ ì œì™¸
            continue
        
        count = row['Count']
        topic_words = topic_model.get_topic(topic_id)
        
        if topic_words:
            # ìƒìœ„ í‚¤ì›Œë“œë¡œ í† í”½ ì´ë¦„ ìƒì„±
            top_words = [word for word, score in topic_words[:3]]
            topic_name = " | ".join(top_words)
            
            # ëª¨ë“  í‚¤ì›Œë“œ ì¶”ê°€
            for word, score in topic_words[:KEYWORDS_PER_TOPIC]:
                topic_keywords.append({
                    'topic_id': topic_id,
                    'topic_name': topic_name,
                    'doc_count': count,
                    'keyword': word,
                    'score': round(score, 4)
                })
    
    # 4. CSV ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    filename = f"bertopic_keywords_{timestamp}.csv"
    
    df = pd.DataFrame(topic_keywords)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    
    # 5. í•™ìŠµ ì§€í‘œ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š [í•™ìŠµ ì§€í‘œ]")
    print("="*60)
    print(f"âœ… ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")
    print(f"âœ… ë°œê²¬ëœ í† í”½: {len(topic_info) - 1}ê°œ (ë…¸ì´ì¦ˆ ì œì™¸)")
    print(f"âœ… ë…¸ì´ì¦ˆ ë¬¸ì„œ: {topic_info[topic_info['Topic'] == -1]['Count'].values[0]}ê°œ")
    print(f"âœ… ë…¸ì´ì¦ˆ ë¹„ìœ¨: {topic_info[topic_info['Topic'] == -1]['Count'].values[0] / len(documents) * 100:.1f}%")
    
    # í† í”½ í¬ê¸° ë¶„í¬
    topic_sizes = topic_info[topic_info['Topic'] != -1]['Count'].values
    print(f"\nğŸ“ˆ í† í”½ í¬ê¸° ë¶„í¬:")
    print(f"   - í‰ê· : {topic_sizes.mean():.1f}ê°œ")
    print(f"   - ìµœëŒ€: {topic_sizes.max()}ê°œ")
    print(f"   - ìµœì†Œ: {topic_sizes.min()}ê°œ")
    
    print("\n" + "="*60)
    print(f"ğŸ‰ BERTopic ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ íŒŒì¼: {filename}")
    print(f"ğŸ”‘ ì´ í‚¤ì›Œë“œ: {len(df)}ê°œ")
    print(f"ğŸ“‚ í† í”½ë³„ í‚¤ì›Œë“œ: {KEYWORDS_PER_TOPIC}ê°œì”©")
    print("="*60)
    
    # ìƒ˜í”Œ ì¶œë ¥ (í† í”½ë³„ë¡œ)
    print(f"\nğŸ“ í‚¤ì›Œë“œ ìƒ˜í”Œ (ìƒìœ„ 3ê°œ í† í”½):")
    for topic_id in topic_info['Topic'].values[1:4]:  # 0ë²ˆì§¸ëŠ” -1 (ë…¸ì´ì¦ˆ)
        topic_df = df[df['topic_id'] == topic_id]
        if not topic_df.empty:
            print(f"\nğŸ”– Topic {topic_id}: {topic_df.iloc[0]['topic_name']}")
            print(f"   ë¬¸ì„œ: {topic_df.iloc[0]['doc_count']}ê°œ")
            print(f"   í‚¤ì›Œë“œ: {', '.join(topic_df['keyword'].head(10).tolist())}")

if __name__ == "__main__":
    main()
